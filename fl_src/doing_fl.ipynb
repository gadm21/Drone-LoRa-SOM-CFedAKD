{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gadmohamed/miniforge3/envs/env_tf/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from data_utils import * \n",
    "from model_utils import *\n",
    "from utils import FLClient, FLServer\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import os\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CIFAR-10 for FL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "(50000, 32, 32, 3) (50000,) (10000, 32, 32, 3) (10000,) (50000, 10) (10000, 10)\n",
      "(50000, 32, 32, 3) (50000,) (10000, 32, 32, 3) (10000,) (50000, 10) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "dataset = 'cifar10'\n",
    "num_classes = 10 if dataset == 'cifar10' else 100 \n",
    "pub_num_classes = 100 if num_classes == 10 else 10\n",
    "datadir = '../data'\n",
    "partition = 'iid' \n",
    "n_parties = 5\n",
    "beta = 0.5\n",
    "\n",
    "(X_train, y_train, X_test, y_test, net_dataidx_map) = partition_data('cifar10', datadir=datadir, partition = partition, n_parties = n_parties, beta = beta)\n",
    "(X_train_public, y_train_public, X_test_public, y_test_public, net_dataidx_map_public) = partition_data('cifar100', datadir=datadir, partition = 'iid', n_parties = 10, beta = 0.5)\n",
    "# divide y_public by 10 to make it compatible with cifar10\n",
    "y_train_public = y_train_public // 10\n",
    "y_test_public = y_test_public // 10\n",
    "\n",
    "y_train_cat = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "# use num_classes instead of pub_num_classes to make it compatible with cifar10\n",
    "y_train_public_cat = to_categorical(y_train_public, num_classes=num_classes)\n",
    "y_test_public_cat = to_categorical(y_test_public, num_classes=num_classes)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, y_train_cat.shape, y_test_cat.shape)\n",
    "print(X_train_public.shape, y_train_public.shape, X_test_public.shape, y_test_public.shape, y_train_public_cat.shape, y_test_public_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 32, 32, 3) (5000, 10)\n",
      "client  0   (10000, 32, 32, 3) (10000, 10)\n",
      "(10000, 32, 32, 3) (10000, 10)\n",
      "\n",
      "client  1   (10000, 32, 32, 3) (10000, 10)\n",
      "(10000, 32, 32, 3) (10000, 10)\n",
      "\n",
      "client  2   (10000, 32, 32, 3) (10000, 10)\n",
      "(10000, 32, 32, 3) (10000, 10)\n",
      "\n",
      "client  3   (10000, 32, 32, 3) (10000, 10)\n",
      "(10000, 32, 32, 3) (10000, 10)\n",
      "\n",
      "client  4   (10000, 32, 32, 3) (10000, 10)\n",
      "(10000, 32, 32, 3) (10000, 10)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "local_sets = [] \n",
    "test_sets = []\n",
    "public_set = (X_train_public[net_dataidx_map_public[0]], y_train_public_cat[net_dataidx_map_public[0]])\n",
    "for i in range(n_parties):\n",
    "    local_sets.append((X_train[net_dataidx_map[i]], y_train_cat[net_dataidx_map[i]]))\n",
    "    test_sets.append((X_test, y_test_cat))\n",
    "    \n",
    "print(public_set[0].shape, public_set[1].shape)\n",
    "for i in range(n_parties):\n",
    "    print('client ', i, ' ', local_sets[i][0].shape, local_sets[i][1].shape)\n",
    "    print(test_sets[i][0].shape, test_sets[i][1].shape)\n",
    "    print() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_method = 'weights'\n",
    "aug = False\n",
    "weighting = 'uniform'\n",
    "private =   False\n",
    "hyperparameter_tuning = False\n",
    "C = 1\n",
    "initial_pub_alignment_epochs = 4\n",
    "\n",
    "fl_params = {\n",
    "    'client_num': 3, \n",
    "    'tot_T': 20, \n",
    "    'C': C,\n",
    "    'local_sets': local_sets,\n",
    "    'test_sets': test_sets,\n",
    "    'public_set': public_set,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 3, \n",
    "    'lr': 0.003,\n",
    "    'aggregate': aggregation_method, # 'grads', 'compressed_soft_labels', 'soft_labels'\n",
    "    'hyperparameter_tuning': hyperparameter_tuning, \n",
    "    'weighting': weighting, # 'uniform', 'performance_based'\n",
    "    'default_client_id': 1, \n",
    "    'augment': aug, \n",
    "    'private': private,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'delta': 1e-4,\n",
    "    'epsilon': 5,\n",
    "    'local_benchmark_epochs': 70,\n",
    "    'initial_pub_alignment_epochs': initial_pub_alignment_epochs\n",
    "}\n",
    "\n",
    "N_pub = len(fl_params['public_set'][0])\n",
    "exp_path = f\"../fl_results/{dataset}/DP{private}/N_pub{N_pub}/Agg{fl_params['aggregate']}_C{fl_params['C']}_HT{fl_params['hyperparameter_tuning']}_Aug{fl_params['augment']}_W{fl_params['weighting']}\"\n",
    "fl_params['exp_path'] = exp_path\n",
    "server = FLServer(fl_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train acc 0.11441693290734824 | Test acc 0.12809504792332269\n",
      "Epoch 1 | Train acc 0.1422723642172524 | Test acc 0.15115814696485624\n",
      "Epoch 2 | Train acc 0.15495207667731628 | Test acc 0.15485223642172524\n",
      "Epoch 0 | Train acc 0.10283546325878594 | Test acc 0.10583067092651757\n",
      "Epoch 1 | Train acc 0.1239017571884984 | Test acc 0.15215654952076677\n",
      "Epoch 2 | Train acc 0.15744808306709265 | Test acc 0.16813099041533547\n",
      "Epoch 0 | Train acc 0.10453274760383387 | Test acc 0.11281948881789138\n",
      "Epoch 1 | Train acc 0.1326876996805112 | Test acc 0.15505191693290735\n",
      "Epoch 2 | Train acc 0.16094249201277955 | Test acc 0.15894568690095848\n",
      "Round 0 : avg_acc 0.16064297124600638, min_acc 0.15485223642172524, max_acc 0.16813099041533547, avg_train_acc 0.15778088391906284\n",
      "Epoch 0 | Train acc 0.10083865814696485 | Test acc 0.09994009584664537\n",
      "Epoch 1 | Train acc 0.10083865814696485 | Test acc 0.10003993610223642\n",
      "Epoch 2 | Train acc 0.10363418530351437 | Test acc 0.10023961661341853\n",
      "Epoch 0 | Train acc 0.10393370607028754 | Test acc 0.09984025559105432\n",
      "Epoch 1 | Train acc 0.10403354632587859 | Test acc 0.09984025559105432\n",
      "Epoch 2 | Train acc 0.10393370607028754 | Test acc 0.09994009584664537\n",
      "Epoch 0 | Train acc 0.09714456869009584 | Test acc 0.10003993610223642\n",
      "Epoch 1 | Train acc 0.09984025559105432 | Test acc 0.10712859424920128\n",
      "Epoch 2 | Train acc 0.13708067092651757 | Test acc 0.14716453674121405\n",
      "Round 1 : avg_acc 0.11578141640042598, min_acc 0.09994009584664537, max_acc 0.14716453674121405, avg_train_acc 0.1148828541001065\n",
      "Epoch 0 | Train acc 0.09734424920127796 | Test acc 0.10013977635782748\n",
      "Epoch 1 | Train acc 0.10253594249201278 | Test acc 0.14476837060702874\n",
      "Epoch 2 | Train acc 0.128694089456869 | Test acc 0.09994009584664537\n",
      "Epoch 0 | Train acc 0.1007388178913738 | Test acc 0.10003993610223642\n",
      "Epoch 1 | Train acc 0.1042332268370607 | Test acc 0.09994009584664537\n",
      "Epoch 2 | Train acc 0.106629392971246 | Test acc 0.10013977635782748\n",
      "Epoch 0 | Train acc 0.10403354632587859 | Test acc 0.10003993610223642\n",
      "Epoch 1 | Train acc 0.10403354632587859 | Test acc 0.09994009584664537\n",
      "Epoch 2 | Train acc 0.10393370607028754 | Test acc 0.09994009584664537\n",
      "Round 2 : avg_acc 0.1000066560170394, min_acc 0.09994009584664537, max_acc 0.10013977635782748, avg_train_acc 0.11308572949946753\n",
      "Epoch 0 | Train acc 0.11591453674121406 | Test acc 0.10103833865814696\n",
      "Epoch 1 | Train acc 0.10962460063897764 | Test acc 0.10003993610223642\n",
      "Epoch 2 | Train acc 0.10672923322683706 | Test acc 0.09994009584664537\n",
      "Epoch 0 | Train acc 0.1295926517571885 | Test acc 0.15335463258785942\n",
      "Epoch 1 | Train acc 0.15245607028753994 | Test acc 0.13358626198083068\n",
      "Epoch 2 | Train acc 0.1418730031948882 | Test acc 0.14197284345047922\n",
      "Epoch 0 | Train acc 0.11242012779552715 | Test acc 0.09994009584664537\n",
      "Epoch 1 | Train acc 0.10463258785942492 | Test acc 0.09994009584664537\n",
      "Epoch 2 | Train acc 0.10393370607028754 | Test acc 0.09984025559105432\n",
      "Round 3 : avg_acc 0.11391773162939296, min_acc 0.09984025559105432, max_acc 0.14197284345047922, avg_train_acc 0.11751198083067094\n",
      "Epoch 0 | Train acc 0.12679712460063897 | Test acc 0.10373402555910544\n",
      "Epoch 1 | Train acc 0.11152156549520767 | Test acc 0.10013977635782748\n",
      "Epoch 2 | Train acc 0.10692891373801917 | Test acc 0.09984025559105432\n",
      "Epoch 0 | Train acc 0.14766373801916932 | Test acc 0.15714856230031948\n",
      "Epoch 1 | Train acc 0.14616613418530353 | Test acc 0.15435303514376997\n",
      "Epoch 2 | Train acc 0.16124201277955272 | Test acc 0.1582468051118211\n",
      "Epoch 0 | Train acc 0.12330271565495207 | Test acc 0.15185702875399362\n",
      "Epoch 1 | Train acc 0.13578274760383385 | Test acc 0.12949281150159744\n",
      "Epoch 2 | Train acc 0.12300319488817892 | Test acc 0.1051317891373802\n",
      "Round 4 : avg_acc 0.12107294994675187, min_acc 0.09984025559105432, max_acc 0.1582468051118211, avg_train_acc 0.13039137380191693\n",
      "Epoch 0 | Train acc 0.1284944089456869 | Test acc 0.1564496805111821\n",
      "Epoch 1 | Train acc 0.14746405750798722 | Test acc 0.15694888178913738\n",
      "Epoch 2 | Train acc 0.1365814696485623 | Test acc 0.1564496805111821\n",
      "Epoch 0 | Train acc 0.14656549520766773 | Test acc 0.12330271565495207\n",
      "Epoch 1 | Train acc 0.12769568690095848 | Test acc 0.10023961661341853\n",
      "Epoch 2 | Train acc 0.10672923322683706 | Test acc 0.09994009584664537\n",
      "Epoch 0 | Train acc 0.1424720447284345 | Test acc 0.13867811501597443\n",
      "Epoch 1 | Train acc 0.14846246006389777 | Test acc 0.15065894568690097\n",
      "Epoch 2 | Train acc 0.14676517571884984 | Test acc 0.14876198083067094\n",
      "Round 5 : avg_acc 0.13505058572949946, min_acc 0.09994009584664537, max_acc 0.1564496805111821, avg_train_acc 0.13002529286474973\n",
      "Epoch 0 | Train acc 0.14297124600638977 | Test acc 0.13907747603833867\n",
      "Epoch 1 | Train acc 0.14017571884984026 | Test acc 0.10942492012779553\n",
      "Epoch 2 | Train acc 0.12090654952076678 | Test acc 0.10083865814696485\n",
      "Epoch 0 | Train acc 0.13947683706070288 | Test acc 0.13957667731629392\n",
      "Epoch 1 | Train acc 0.14626597444089456 | Test acc 0.14906150159744408\n",
      "Epoch 2 | Train acc 0.14307108626198084 | Test acc 0.15025958466453673\n",
      "Epoch 0 | Train acc 0.1503594249201278 | Test acc 0.15575079872204473\n",
      "Epoch 1 | Train acc 0.15265575079872204 | Test acc 0.15625\n",
      "Epoch 2 | Train acc 0.14586661341853036 | Test acc 0.16363817891373802\n",
      "Round 6 : avg_acc 0.1382454739084132, min_acc 0.10083865814696485, max_acc 0.16363817891373802, avg_train_acc 0.13661474973375934\n",
      "Epoch 0 | Train acc 0.16184105431309903 | Test acc 0.16034345047923323\n",
      "Epoch 1 | Train acc 0.16383785942492013 | Test acc 0.16733226837060702\n",
      "Epoch 2 | Train acc 0.1542531948881789 | Test acc 0.18280750798722045\n",
      "Epoch 0 | Train acc 0.15854632587859424 | Test acc 0.14666533546325877\n",
      "Epoch 1 | Train acc 0.1457667731629393 | Test acc 0.12130591054313099\n",
      "Epoch 2 | Train acc 0.11811102236421725 | Test acc 0.1110223642172524\n",
      "Epoch 0 | Train acc 0.15415335463258786 | Test acc 0.1472643769968051\n",
      "Epoch 1 | Train acc 0.1479632587859425 | Test acc 0.15195686900958466\n",
      "Epoch 2 | Train acc 0.1586461661341853 | Test acc 0.15245607028753994\n",
      "Round 7 : avg_acc 0.14876198083067094, min_acc 0.1110223642172524, max_acc 0.18280750798722045, avg_train_acc 0.14367012779552715\n",
      "Epoch 0 | Train acc 0.17531948881789136 | Test acc 0.18081070287539935\n",
      "Epoch 1 | Train acc 0.17791533546325877 | Test acc 0.18021166134185304\n",
      "Epoch 2 | Train acc 0.18590255591054314 | Test acc 0.18001198083067094\n",
      "Epoch 0 | Train acc 0.1739217252396166 | Test acc 0.16293929712460065\n",
      "Epoch 1 | Train acc 0.15984424920127796 | Test acc 0.14756389776357828\n",
      "Epoch 2 | Train acc 0.1516573482428115 | Test acc 0.14157348242811502\n",
      "Epoch 0 | Train acc 0.17492012779552715 | Test acc 0.18350638977635783\n",
      "Epoch 1 | Train acc 0.1829073482428115 | Test acc 0.1959864217252396\n",
      "Epoch 2 | Train acc 0.19738418530351437 | Test acc 0.20617012779552715\n",
      "Round 8 : avg_acc 0.1759185303514377, min_acc 0.14157348242811502, max_acc 0.20617012779552715, avg_train_acc 0.178314696485623\n",
      "Epoch 0 | Train acc 0.1876996805111821 | Test acc 0.19149361022364217\n",
      "Epoch 1 | Train acc 0.18590255591054314 | Test acc 0.18740015974440893\n",
      "Epoch 2 | Train acc 0.18979632587859424 | Test acc 0.1900958466453674\n",
      "Epoch 0 | Train acc 0.19628594249201278 | Test acc 0.20007987220447285\n",
      "Epoch 1 | Train acc 0.20836661341853036 | Test acc 0.2036741214057508\n",
      "Epoch 2 | Train acc 0.1955870607028754 | Test acc 0.20397364217252395\n",
      "Epoch 0 | Train acc 0.1990814696485623 | Test acc 0.19259185303514376\n",
      "Epoch 1 | Train acc 0.19199281150159744 | Test acc 0.1873003194888179\n",
      "Epoch 2 | Train acc 0.1849041533546326 | Test acc 0.18390575079872204\n",
      "Round 9 : avg_acc 0.1926584132055378, min_acc 0.18390575079872204, max_acc 0.20397364217252395, avg_train_acc 0.1900958466453674\n",
      "Epoch 0 | Train acc 0.19349041533546327 | Test acc 0.18979632587859424\n",
      "Epoch 1 | Train acc 0.18799920127795527 | Test acc 0.1878993610223642\n",
      "Epoch 2 | Train acc 0.1885982428115016 | Test acc 0.1878993610223642\n",
      "Epoch 0 | Train acc 0.20157747603833867 | Test acc 0.19828274760383385\n",
      "Epoch 1 | Train acc 0.19458865814696485 | Test acc 0.1951876996805112\n",
      "Epoch 2 | Train acc 0.1871006389776358 | Test acc 0.18919728434504793\n",
      "Epoch 0 | Train acc 0.20097843450479233 | Test acc 0.1992811501597444\n",
      "Epoch 1 | Train acc 0.19249201277955272 | Test acc 0.19089456869009586\n",
      "Epoch 2 | Train acc 0.18819888178913738 | Test acc 0.18061102236421725\n",
      "Round 10 : avg_acc 0.18590255591054314, min_acc 0.18061102236421725, max_acc 0.18919728434504793, avg_train_acc 0.18796592119275823\n",
      "Epoch 0 | Train acc 0.18170926517571884 | Test acc 0.17422124600638977\n",
      "Epoch 1 | Train acc 0.1792132587859425 | Test acc 0.17202476038338657\n",
      "Epoch 2 | Train acc 0.17951277955271566 | Test acc 0.18310702875399362\n",
      "Epoch 0 | Train acc 0.1798123003194888 | Test acc 0.18280750798722045\n",
      "Epoch 1 | Train acc 0.18460463258785942 | Test acc 0.18340654952076677\n",
      "Epoch 2 | Train acc 0.18580271565495207 | Test acc 0.19059504792332269\n",
      "Epoch 0 | Train acc 0.18150958466453673 | Test acc 0.1790135782747604\n",
      "Epoch 1 | Train acc 0.17671725239616615 | Test acc 0.18091054313099042\n",
      "Epoch 2 | Train acc 0.18620207667731628 | Test acc 0.18899760383386582\n",
      "Round 11 : avg_acc 0.18756656017039405, min_acc 0.18310702875399362, max_acc 0.19059504792332269, avg_train_acc 0.183839190628328\n",
      "Epoch 0 | Train acc 0.19418929712460065 | Test acc 0.2016773162939297\n",
      "Epoch 1 | Train acc 0.2069688498402556 | Test acc 0.2141573482428115\n",
      "Epoch 2 | Train acc 0.22044728434504793 | Test acc 0.21884984025559107\n",
      "Epoch 0 | Train acc 0.196685303514377 | Test acc 0.20137779552715654\n",
      "Epoch 1 | Train acc 0.21435702875399362 | Test acc 0.20686900958466453\n",
      "Epoch 2 | Train acc 0.22304313099041534 | Test acc 0.22733626198083068\n",
      "Epoch 0 | Train acc 0.19149361022364217 | Test acc 0.21435702875399362\n",
      "Epoch 1 | Train acc 0.22084664536741214 | Test acc 0.2233426517571885\n",
      "Epoch 2 | Train acc 0.22893370607028754 | Test acc 0.2384185303514377\n",
      "Round 12 : avg_acc 0.22820154419595315, min_acc 0.21884984025559107, max_acc 0.2384185303514377, avg_train_acc 0.22414137380191693\n",
      "Epoch 0 | Train acc 0.22863418530351437 | Test acc 0.22943290734824281\n",
      "Epoch 1 | Train acc 0.23182907348242812 | Test acc 0.2430111821086262\n",
      "Epoch 2 | Train acc 0.23612220447284346 | Test acc 0.2515974440894569\n",
      "Epoch 0 | Train acc 0.23652156549520767 | Test acc 0.2384185303514377\n",
      "Epoch 1 | Train acc 0.24071485623003194 | Test acc 0.23242811501597443\n",
      "Epoch 2 | Train acc 0.24251198083067094 | Test acc 0.2514976038338658\n",
      "Epoch 0 | Train acc 0.23722044728434505 | Test acc 0.23212859424920126\n",
      "Epoch 1 | Train acc 0.2415135782747604 | Test acc 0.2510982428115016\n",
      "Epoch 2 | Train acc 0.2510982428115016 | Test acc 0.25549121405750796\n",
      "Round 13 : avg_acc 0.25286208732694354, min_acc 0.2514976038338658, max_acc 0.25549121405750796, avg_train_acc 0.24324414270500536\n",
      "Epoch 0 | Train acc 0.26347843450479236 | Test acc 0.2510982428115016\n",
      "Epoch 1 | Train acc 0.2608825878594249 | Test acc 0.2714656549520767\n",
      "Epoch 2 | Train acc 0.27286341853035145 | Test acc 0.26936900958466453\n",
      "Epoch 0 | Train acc 0.2521964856230032 | Test acc 0.25778753993610226\n",
      "Epoch 1 | Train acc 0.25469249201277955 | Test acc 0.26407747603833864\n",
      "Epoch 2 | Train acc 0.2633785942492013 | Test acc 0.273861821086262\n",
      "Epoch 0 | Train acc 0.2489017571884984 | Test acc 0.25868610223642174\n",
      "Epoch 1 | Train acc 0.25559105431309903 | Test acc 0.26317891373801916\n",
      "Epoch 2 | Train acc 0.26148162939297126 | Test acc 0.26936900958466453\n",
      "Round 14 : avg_acc 0.2708666134185304, min_acc 0.26936900958466453, max_acc 0.273861821086262, avg_train_acc 0.26590788072417465\n",
      "Epoch 0 | Train acc 0.2763578274760383 | Test acc 0.2685702875399361\n",
      "Epoch 1 | Train acc 0.2817492012779553 | Test acc 0.2882388178913738\n",
      "Epoch 2 | Train acc 0.2862420127795527 | Test acc 0.29073482428115016\n",
      "Epoch 0 | Train acc 0.270267571884984 | Test acc 0.26737220447284343\n",
      "Epoch 1 | Train acc 0.27815495207667734 | Test acc 0.27935303514376997\n",
      "Epoch 2 | Train acc 0.28025159744408945 | Test acc 0.2816493610223642\n",
      "Epoch 0 | Train acc 0.26996805111821087 | Test acc 0.2565894568690096\n",
      "Epoch 1 | Train acc 0.27136581469648563 | Test acc 0.2860423322683706\n",
      "Epoch 2 | Train acc 0.2845447284345048 | Test acc 0.28424520766773165\n",
      "Round 15 : avg_acc 0.28554313099041534, min_acc 0.2816493610223642, max_acc 0.29073482428115016, avg_train_acc 0.28367944621938235\n",
      "Epoch 0 | Train acc 0.28434504792332266 | Test acc 0.2965255591054313\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/gadmohamed/Desktop/live repos/Drone-LoRa-SOM-CFedAKD/fl_src/doing_fl.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gadmohamed/Desktop/live%20repos/Drone-LoRa-SOM-CFedAKD/fl_src/doing_fl.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m rr \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(fl_params[\u001b[39m'\u001b[39m\u001b[39mtot_T\u001b[39m\u001b[39m'\u001b[39m]) : \n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gadmohamed/Desktop/live%20repos/Drone-LoRa-SOM-CFedAKD/fl_src/doing_fl.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     avg_acc, min_acc, max_acc, avg_train_acc \u001b[39m=\u001b[39m server\u001b[39m.\u001b[39;49mglobal_update(verbose \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gadmohamed/Desktop/live%20repos/Drone-LoRa-SOM-CFedAKD/fl_src/doing_fl.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRound \u001b[39m\u001b[39m{\u001b[39;00mrr\u001b[39m}\u001b[39;00m\u001b[39m : avg_acc \u001b[39m\u001b[39m{\u001b[39;00mavg_acc\u001b[39m}\u001b[39;00m\u001b[39m, min_acc \u001b[39m\u001b[39m{\u001b[39;00mmin_acc\u001b[39m}\u001b[39;00m\u001b[39m, max_acc \u001b[39m\u001b[39m{\u001b[39;00mmax_acc\u001b[39m}\u001b[39;00m\u001b[39m, avg_train_acc \u001b[39m\u001b[39m{\u001b[39;00mavg_train_acc\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/live repos/Drone-LoRa-SOM-CFedAKD/fl_src/utils.py:878\u001b[0m, in \u001b[0;36mFLServer.global_update\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m    874\u001b[0m idxs_users \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclients)), \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mC \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclients)), replace\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)        \n\u001b[1;32m    876\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m idxs_users:\n\u001b[1;32m    877\u001b[0m     \u001b[39m# print(\"client {} accuracy before: {}\".format(idx, self.clients[idx].get_test_acc()))\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclients[idx]\u001b[39m.\u001b[39;49mlocal_update(verbose \u001b[39m=\u001b[39;49m verbose)\n\u001b[1;32m    879\u001b[0m     \u001b[39m# print(\"client {} accuracy after: {} \".format(idx, self.clients[idx].get_test_acc()))\u001b[39;00m\n\u001b[1;32m    882\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39maggregate\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39msoft_labels\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/live repos/Drone-LoRa-SOM-CFedAKD/fl_src/utils.py:646\u001b[0m, in \u001b[0;36mFLClient.local_update\u001b[0;34m(self, evaluate, verbose)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocal_dl:\n\u001b[1;32m    644\u001b[0m     x, y \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m--> 646\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x)\n\u001b[1;32m    647\u001b[0m     probs \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(logits, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    648\u001b[0m     log_probs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlog(probs)\n",
      "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/live repos/Drone-LoRa-SOM-CFedAKD/fl_src/model_utils.py:296\u001b[0m, in \u001b[0;36mNet_CIFAR.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    295\u001b[0m     \u001b[39m# x = x.permute(0, 3, 1, 2)\u001b[39;00m\n\u001b[0;32m--> 296\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)))\n\u001b[1;32m    297\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x)))\n\u001b[1;32m    298\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3(x)))\n",
      "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for rr in range(fl_params['tot_T']) : \n",
    "    avg_acc, min_acc, max_acc, avg_train_acc = server.global_update(verbose = True)\n",
    "    print(f\"Round {rr} : avg_acc {avg_acc}, min_acc {min_acc}, max_acc {max_acc}, avg_train_acc {avg_train_acc}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# server = FLServer(fl_params)\n",
    "# for i in range(4) : \n",
    "#     accs, losses = train(server.clients[0].model, server.clients[0].local_dl, server.clients[0].optimizer)\n",
    "#     print(f\"Round {i} : accs {accs}, losses {losses}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_method = 'compressed_soft_labels'\n",
    "aug = False\n",
    "weighting = 'uniform'\n",
    "private =   False\n",
    "hyperparameter_tuning = False\n",
    "C = 1\n",
    "initial_pub_alignment_epochs = 4 \n",
    "\n",
    "fl_params = {\n",
    "    'client_num': 3, \n",
    "    'tot_T': 20, \n",
    "    'C': C,\n",
    "    'local_sets': local_sets,\n",
    "    'test_sets': test_sets,\n",
    "    'public_set': public_set,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 3, \n",
    "    'lr': 0.003,\n",
    "    'aggregate': aggregation_method, # 'grads', 'compressed_soft_labels', 'soft_labels'\n",
    "    'hyperparameter_tuning': hyperparameter_tuning, \n",
    "    'weighting': weighting, # 'uniform', 'performance_based'\n",
    "    'default_client_id': 1, \n",
    "    'augment': aug, \n",
    "    'private': private,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'delta': 1e-4,\n",
    "    'epsilon': 5,\n",
    "    'local_benchmark_epochs': 70,\n",
    "    'initial_pub_alignment_epochs': initial_pub_alignment_epochs, \n",
    "    'temperature': 0.6, \n",
    "}\n",
    "\n",
    "N_pub = len(fl_params['public_set'][0])\n",
    "exp_path = f\"../fl_results/{dataset}/DP{private}/N_pub{N_pub}/Agg{fl_params['aggregate']}_C{fl_params['C']}_HT{fl_params['hyperparameter_tuning']}_Aug{fl_params['augment']}_W{fl_params['weighting']}\"\n",
    "fl_params['exp_path'] = exp_path\n",
    "soft_labels_server = FLServer(fl_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import size_of \n",
    "soft_labels = server.clients[0].get_soft_labels()\n",
    "compressed_soft_labels = soft_labels_server.clients[0].get_soft_labels(normalize = True, compress = True)\n",
    "clientmodel = server.clients[0].model\n",
    "ssize = size_of(soft_labels, size = 'KB')\n",
    "cssize = size_of(compressed_soft_labels, size = 'KB')\n",
    "clientmodel_state = clientmodel.state_dict()\n",
    "msize = size_of(clientmodel_state, size = 'KB')\n",
    "\n",
    "cssize, ssize, msize \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train acc 0.09644568690095846 | Test acc 0.09574680511182108\n",
      "Epoch 1 | Train acc 0.10812699680511183 | Test acc 0.11112220447284345\n",
      "Epoch 2 | Train acc 0.11950878594249201 | Test acc 0.12190495207667731\n",
      "Epoch 0 | Train acc 0.130491214057508 | Test acc 0.16383785942492013\n",
      "Epoch 1 | Train acc 0.1796126198083067 | Test acc 0.1876996805111821\n",
      "Epoch 2 | Train acc 0.19349041533546327 | Test acc 0.19418929712460065\n",
      "Epoch 0 | Train acc 0.10013977635782748 | Test acc 0.11671325878594249\n",
      "Epoch 1 | Train acc 0.1387779552715655 | Test acc 0.15575079872204473\n",
      "Epoch 2 | Train acc 0.1706269968051118 | Test acc 0.191194089456869\n",
      "Round 0 : avg_acc 0.16909611288604898, min_acc 0.12190495207667731, max_acc 0.19418929712460065, avg_train_acc 0.16120873269435568\n",
      "Epoch 0 | Train acc 0.19818290734824281 | Test acc 0.20746805111821087\n",
      "Epoch 1 | Train acc 0.20666932907348243 | Test acc 0.20766773162939298\n",
      "Epoch 2 | Train acc 0.2097643769968051 | Test acc 0.21475638977635783\n",
      "Epoch 0 | Train acc 0.19179313099041534 | Test acc 0.18899760383386582\n",
      "Epoch 1 | Train acc 0.18849840255591055 | Test acc 0.18889776357827476\n",
      "Epoch 2 | Train acc 0.18650159744408945 | Test acc 0.18909744408945686\n",
      "Epoch 0 | Train acc 0.14616613418530353 | Test acc 0.16693290734824281\n",
      "Epoch 1 | Train acc 0.1898961661341853 | Test acc 0.2250399361022364\n",
      "Epoch 2 | Train acc 0.22474041533546327 | Test acc 0.22863418530351437\n",
      "Round 1 : avg_acc 0.2108293397231097, min_acc 0.18909744408945686, max_acc 0.22863418530351437, avg_train_acc 0.2070021299254526\n",
      "Epoch 0 | Train acc 0.18460463258785942 | Test acc 0.20547124600638977\n",
      "Epoch 1 | Train acc 0.21236022364217252 | Test acc 0.22164536741214058\n",
      "Epoch 2 | Train acc 0.2233426517571885 | Test acc 0.23941693290734825\n",
      "Epoch 0 | Train acc 0.22983226837060702 | Test acc 0.23282747603833867\n",
      "Epoch 1 | Train acc 0.23302715654952078 | Test acc 0.24640575079872204\n",
      "Epoch 2 | Train acc 0.25359424920127793 | Test acc 0.2633785942492013\n",
      "Epoch 0 | Train acc 0.21695287539936103 | Test acc 0.22044728434504793\n",
      "Epoch 1 | Train acc 0.2356230031948882 | Test acc 0.2523961661341853\n",
      "Epoch 2 | Train acc 0.2519968051118211 | Test acc 0.2605830670926518\n",
      "Round 2 : avg_acc 0.25445953141640043, min_acc 0.23941693290734825, max_acc 0.2633785942492013, avg_train_acc 0.24297790202342917\n",
      "Epoch 0 | Train acc 0.2641773162939297 | Test acc 0.2788538338658147\n",
      "Epoch 1 | Train acc 0.27565894568690097 | Test acc 0.28424520766773165\n",
      "Epoch 2 | Train acc 0.2840455271565495 | Test acc 0.2982228434504792\n",
      "Epoch 0 | Train acc 0.24550718849840256 | Test acc 0.25668929712460065\n",
      "Epoch 1 | Train acc 0.2534944089456869 | Test acc 0.2565894568690096\n",
      "Epoch 2 | Train acc 0.26777156549520764 | Test acc 0.2727635782747604\n",
      "Epoch 0 | Train acc 0.26916932907348246 | Test acc 0.2678714057507987\n",
      "Epoch 1 | Train acc 0.2784544728434505 | Test acc 0.2852436102236422\n",
      "Epoch 2 | Train acc 0.28504392971246006 | Test acc 0.292232428115016\n",
      "Round 3 : avg_acc 0.2877396166134185, min_acc 0.2727635782747604, max_acc 0.2982228434504792, avg_train_acc 0.27895367412140576\n",
      "Epoch 0 | Train acc 0.29642571884984026 | Test acc 0.3031150159744409\n",
      "Epoch 1 | Train acc 0.3019169329073482 | Test acc 0.31609424920127793\n",
      "Epoch 2 | Train acc 0.3192891373801917 | Test acc 0.30990415335463256\n",
      "Epoch 0 | Train acc 0.2946285942492013 | Test acc 0.2985223642172524\n",
      "Epoch 1 | Train acc 0.299620607028754 | Test acc 0.3016174121405751\n",
      "Epoch 2 | Train acc 0.3125 | Test acc 0.31190095846645366\n",
      "Epoch 0 | Train acc 0.28065095846645366 | Test acc 0.28464456869009586\n",
      "Epoch 1 | Train acc 0.28963658146964855 | Test acc 0.2971246006389776\n",
      "Epoch 2 | Train acc 0.29802316293929715 | Test acc 0.30131789137380194\n",
      "Round 4 : avg_acc 0.3077076677316294, min_acc 0.30131789137380194, max_acc 0.31190095846645366, avg_train_acc 0.30993743343982966\n",
      "Epoch 0 | Train acc 0.31609424920127793 | Test acc 0.3222843450479233\n",
      "Epoch 1 | Train acc 0.32368210862619806 | Test acc 0.33226837060702874\n",
      "Epoch 2 | Train acc 0.3347643769968051 | Test acc 0.3336661341853035\n",
      "Epoch 0 | Train acc 0.2987220447284345 | Test acc 0.3148961661341853\n",
      "Epoch 1 | Train acc 0.3092052715654952 | Test acc 0.3131988817891374\n",
      "Epoch 2 | Train acc 0.3174920127795527 | Test acc 0.32238418530351437\n",
      "Epoch 0 | Train acc 0.3146964856230032 | Test acc 0.31699281150159747\n",
      "Epoch 1 | Train acc 0.3275758785942492 | Test acc 0.3262779552715655\n",
      "Epoch 2 | Train acc 0.33616214057507987 | Test acc 0.33007188498402557\n",
      "Round 5 : avg_acc 0.32870740149094785, min_acc 0.32238418530351437, max_acc 0.3336661341853035, avg_train_acc 0.3294728434504792\n",
      "Epoch 0 | Train acc 0.32897364217252395 | Test acc 0.3408546325878594\n",
      "Epoch 1 | Train acc 0.3314696485623003 | Test acc 0.34195287539936103\n",
      "Epoch 2 | Train acc 0.34075479233226835 | Test acc 0.3501397763578275\n",
      "Epoch 0 | Train acc 0.3373602236421725 | Test acc 0.3360623003194888\n",
      "Epoch 1 | Train acc 0.3479432907348243 | Test acc 0.34065495207667734\n",
      "Epoch 2 | Train acc 0.3509384984025559 | Test acc 0.3608226837060703\n",
      "Epoch 0 | Train acc 0.3384584664536741 | Test acc 0.33336661341853036\n",
      "Epoch 1 | Train acc 0.3448482428115016 | Test acc 0.35583067092651754\n",
      "Epoch 2 | Train acc 0.36311900958466453 | Test acc 0.34764376996805113\n",
      "Round 6 : avg_acc 0.35286874334398294, min_acc 0.34764376996805113, max_acc 0.3608226837060703, avg_train_acc 0.3516041001064963\n",
      "Epoch 0 | Train acc 0.3597244408945687 | Test acc 0.3620207667731629\n",
      "Epoch 1 | Train acc 0.36701277955271566 | Test acc 0.3610223642172524\n",
      "Epoch 2 | Train acc 0.37639776357827476 | Test acc 0.38248801916932906\n",
      "Epoch 0 | Train acc 0.3600239616613419 | Test acc 0.36421725239616615\n",
      "Epoch 1 | Train acc 0.367711661341853 | Test acc 0.36092252396166136\n",
      "Epoch 2 | Train acc 0.3743011182108626 | Test acc 0.3745007987220447\n",
      "Epoch 0 | Train acc 0.34634584664536744 | Test acc 0.3591253993610224\n",
      "Epoch 1 | Train acc 0.34974041533546324 | Test acc 0.3506389776357827\n",
      "Epoch 2 | Train acc 0.36022364217252395 | Test acc 0.3698083067092652\n",
      "Round 7 : avg_acc 0.37559904153354634, min_acc 0.3698083067092652, max_acc 0.38248801916932906, avg_train_acc 0.37030750798722045\n",
      "Epoch 0 | Train acc 0.38937699680511184 | Test acc 0.3814896166134185\n",
      "Epoch 1 | Train acc 0.39057507987220447 | Test acc 0.3891773162939297\n",
      "Epoch 2 | Train acc 0.3993610223642173 | Test acc 0.39436900958466453\n",
      "Epoch 0 | Train acc 0.36721246006389774 | Test acc 0.3704073482428115\n",
      "Epoch 1 | Train acc 0.3698083067092652 | Test acc 0.37559904153354634\n",
      "Epoch 2 | Train acc 0.37879392971246006 | Test acc 0.36990814696485624\n",
      "Epoch 0 | Train acc 0.3782947284345048 | Test acc 0.36571485623003197\n",
      "Epoch 1 | Train acc 0.38428514376996803 | Test acc 0.38428514376996803\n",
      "Epoch 2 | Train acc 0.3882787539936102 | Test acc 0.3887779552715655\n",
      "Round 8 : avg_acc 0.3843517039403621, min_acc 0.36990814696485624, max_acc 0.39436900958466453, avg_train_acc 0.38881123535676254\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/gadmohamed/Desktop/live repos/Drone-LoRa-SOM-CFedAKD/fl_src/doing_fl.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gadmohamed/Desktop/live%20repos/Drone-LoRa-SOM-CFedAKD/fl_src/doing_fl.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m rr \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m20\u001b[39m) : \n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gadmohamed/Desktop/live%20repos/Drone-LoRa-SOM-CFedAKD/fl_src/doing_fl.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     avg_acc, min_acc, max_acc, avg_train_acc \u001b[39m=\u001b[39m soft_labels_server\u001b[39m.\u001b[39;49mglobal_update(verbose \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m) \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gadmohamed/Desktop/live%20repos/Drone-LoRa-SOM-CFedAKD/fl_src/doing_fl.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRound \u001b[39m\u001b[39m{\u001b[39;00mrr\u001b[39m}\u001b[39;00m\u001b[39m : avg_acc \u001b[39m\u001b[39m{\u001b[39;00mavg_acc\u001b[39m}\u001b[39;00m\u001b[39m, min_acc \u001b[39m\u001b[39m{\u001b[39;00mmin_acc\u001b[39m}\u001b[39;00m\u001b[39m, max_acc \u001b[39m\u001b[39m{\u001b[39;00mmax_acc\u001b[39m}\u001b[39;00m\u001b[39m, avg_train_acc \u001b[39m\u001b[39m{\u001b[39;00mavg_train_acc\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/live repos/Drone-LoRa-SOM-CFedAKD/fl_src/utils.py:878\u001b[0m, in \u001b[0;36mFLServer.global_update\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m    874\u001b[0m idxs_users \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclients)), \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mC \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclients)), replace\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)        \n\u001b[1;32m    876\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m idxs_users:\n\u001b[1;32m    877\u001b[0m     \u001b[39m# print(\"client {} accuracy before: {}\".format(idx, self.clients[idx].get_test_acc()))\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclients[idx]\u001b[39m.\u001b[39;49mlocal_update(verbose \u001b[39m=\u001b[39;49m verbose)\n\u001b[1;32m    879\u001b[0m     \u001b[39m# print(\"client {} accuracy after: {} \".format(idx, self.clients[idx].get_test_acc()))\u001b[39;00m\n\u001b[1;32m    882\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39maggregate\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39msoft_labels\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/live repos/Drone-LoRa-SOM-CFedAKD/fl_src/utils.py:646\u001b[0m, in \u001b[0;36mFLClient.local_update\u001b[0;34m(self, evaluate, verbose)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocal_dl:\n\u001b[1;32m    644\u001b[0m     x, y \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m--> 646\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x)\n\u001b[1;32m    647\u001b[0m     probs \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(logits, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    648\u001b[0m     log_probs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlog(probs)\n",
      "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/live repos/Drone-LoRa-SOM-CFedAKD/fl_src/model_utils.py:297\u001b[0m, in \u001b[0;36mNet_CIFAR.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    295\u001b[0m     \u001b[39m# x = x.permute(0, 3, 1, 2)\u001b[39;00m\n\u001b[1;32m    296\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)))\n\u001b[0;32m--> 297\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv2(x)))\n\u001b[1;32m    298\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3(x)))\n\u001b[1;32m    299\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m128\u001b[39m \u001b[39m*\u001b[39m \u001b[39m4\u001b[39m \u001b[39m*\u001b[39m \u001b[39m4\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for rr in range(20) : \n",
    "    avg_acc, min_acc, max_acc, avg_train_acc = soft_labels_server.global_update(verbose = True) \n",
    "    print(f\"Round {rr} : avg_acc {avg_acc}, min_acc {min_acc}, max_acc {max_acc}, avg_train_acc {avg_train_acc}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FedAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_method = 'weights'\n",
    "aug = False\n",
    "weighting = 'uniform'\n",
    "private =   False\n",
    "hyperparameter_tuning = False\n",
    "C = 1\n",
    "\n",
    "fl_params = {\n",
    "    'client_num': len(local_sets),\n",
    "    'tot_T': 50, \n",
    "    'C': C,\n",
    "    'local_sets': local_sets,\n",
    "    'test_sets': test_sets,\n",
    "    'public_set': public_set,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 3, \n",
    "    'lr': 0.005,\n",
    "    'aggregate': aggregation_method, # 'grads', 'compressed_soft_labels', 'soft_labels'\n",
    "    'hyperparameter_tuning': hyperparameter_tuning, \n",
    "    'weighting': weighting, # 'uniform', 'performance_based'\n",
    "    'default_client_id': 1, \n",
    "    'augment': aug, \n",
    "    'private': private,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'delta': 1e-4,\n",
    "    'epsilon': 5,\n",
    "    'local_benchmark_epochs': 70\n",
    "}\n",
    "\n",
    "N_pub = len(fl_params['public_set'][0])\n",
    "exp_path = f\"../fl_results/{dataset}/DP{private}/N_pub{N_pub}/Agg{fl_params['aggregate']}_C{fl_params['C']}_HT{fl_params['hyperparameter_tuning']}_Aug{fl_params['augment']}_W{fl_params['weighting']}\"\n",
    "fl_params['exp_path'] = exp_path\n",
    "server = FLServer(fl_params)\n",
    "\n",
    "FL_acc = []\n",
    "for t in range(fl_params['tot_T']):\n",
    "    avg_acc, min_acc, max_acc, avg_train_acc = server.global_update()\n",
    "    FL_acc.append(avg_acc)\n",
    "    print(f\"Round {t} accuracy: avg: {avg_acc} min: {min_acc}  max: {max_acc} avg_train: {avg_train_acc}\")\n",
    "print(\"Final accuracy: \", FL_acc[-1])\n",
    "print() \n",
    "\n",
    "client_accs = []\n",
    "for c, client in enumerate(server.clients):\n",
    "    acc = client.local_benchmark()\n",
    "    client_accs.append(acc)\n",
    "    print(f\"Client {c} local benchmark accuracy: {acc}\")\n",
    "print(\"Client accuracies: \", client_accs, \"  \", np.mean(client_accs))\n",
    "print()\n",
    "\n",
    "server.save_assets()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FedMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_method = 'soft_labels'\n",
    "aug = False\n",
    "weighting = 'uniform'\n",
    "private =   False\n",
    "hyperparameter_tuning = False\n",
    "C = 1\n",
    "\n",
    "\n",
    "fl_params = {\n",
    "    'client_num': len(local_sets),\n",
    "    'tot_T': 50, \n",
    "    'C': C,\n",
    "    'local_sets': local_sets,\n",
    "    'test_sets': test_sets,\n",
    "    'public_set': public_set,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 3, \n",
    "    'lr': 0.005,\n",
    "    'aggregate': aggregation_method, # 'grads', 'compressed_soft_labels', 'soft_labels'\n",
    "    'hyperparameter_tuning': hyperparameter_tuning, \n",
    "    'weighting': weighting, # 'uniform', 'performance_based'\n",
    "    'default_client_id': 1, \n",
    "    'augment': aug, \n",
    "    'private': private,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'delta': 1e-4,\n",
    "    'epsilon': 5,\n",
    "    'local_benchmark_epochs': 70\n",
    "}\n",
    "\n",
    "N_pub = len(fl_params['public_set'][0])\n",
    "exp_path = f\"../fl_results/{dataset}/DP{private}/N_pub{N_pub}/Agg{fl_params['aggregate']}_C{fl_params['C']}_HT{fl_params['hyperparameter_tuning']}_Aug{fl_params['augment']}_W{fl_params['weighting']}\"\n",
    "fl_params['exp_path'] = exp_path\n",
    "server = FLServer(fl_params)\n",
    "\n",
    "FL_acc = []\n",
    "for t in range(fl_params['tot_T']):\n",
    "    avg_acc, min_acc, max_acc, avg_train_acc = server.global_update()\n",
    "    FL_acc.append(avg_acc)\n",
    "    print(f\"Round {t} accuracy: avg: {avg_acc} min: {min_acc}  max: {max_acc} avg_train: {avg_train_acc}\")\n",
    "print(\"Final accuracy: \", FL_acc[-1])\n",
    "print() \n",
    "\n",
    "# client_accs = []\n",
    "# for c, client in enumerate(server.clients):\n",
    "#     acc = client.local_benchmark()\n",
    "#     client_accs.append(acc)\n",
    "#     print(f\"Client {c} local benchmark accuracy: {acc}\")\n",
    "# print(\"Client accuracies: \", client_accs, \"  \", np.mean(client_accs))\n",
    "# print()\n",
    "\n",
    "server.save_assets()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CFedAKD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_method = 'compressed_soft_labels'\n",
    "aug = True\n",
    "weighting = 'uniform'\n",
    "private =   False\n",
    "hyperparameter_tuning = False\n",
    "C = 1\n",
    "\n",
    "\n",
    "fl_params = {\n",
    "    'client_num': len(local_sets),\n",
    "    'tot_T': 50, \n",
    "    'C': C,\n",
    "    'local_sets': local_sets,\n",
    "    'test_sets': test_sets,\n",
    "    'public_set': public_set,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 3, \n",
    "    'lr': 0.005,\n",
    "    'aggregate': aggregation_method, # 'grads', 'compressed_soft_labels', 'soft_labels'\n",
    "    'hyperparameter_tuning': hyperparameter_tuning, \n",
    "    'weighting': weighting, # 'uniform', 'performance_based'\n",
    "    'default_client_id': 1, \n",
    "    'augment': aug, \n",
    "    'private': private,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'delta': 1e-4,\n",
    "    'epsilon': 5,\n",
    "    'local_benchmark_epochs': 70\n",
    "}\n",
    "\n",
    "N_pub = len(fl_params['public_set'][0])\n",
    "exp_path = f\"../fl_results/{dataset}/DP{private}/N_pub{N_pub}/Agg{fl_params['aggregate']}_C{fl_params['C']}_HT{fl_params['hyperparameter_tuning']}_Aug{fl_params['augment']}_W{fl_params['weighting']}\"\n",
    "fl_params['exp_path'] = exp_path\n",
    "server = FLServer(fl_params)\n",
    "\n",
    "FL_acc = []\n",
    "for t in range(fl_params['tot_T']):\n",
    "    avg_acc, min_acc, max_acc, avg_train_acc = server.global_update()\n",
    "    FL_acc.append(avg_acc)\n",
    "    print(f\"Round {t} accuracy: avg: {avg_acc} min: {min_acc}  max: {max_acc} avg_train: {avg_train_acc}\")\n",
    "print(\"Final accuracy: \", FL_acc[-1])\n",
    "print() \n",
    "\n",
    "# client_accs = []\n",
    "# for c, client in enumerate(server.clients):\n",
    "#     acc = client.local_benchmark()\n",
    "#     client_accs.append(acc)\n",
    "#     print(f\"Client {c} local benchmark accuracy: {acc}\")\n",
    "# print(\"Client accuracies: \", client_accs, \"  \", np.mean(client_accs))\n",
    "# print()\n",
    "\n",
    "server.save_assets()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FedAKD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_method = 'soft_labels'\n",
    "aug = True\n",
    "weighting = 'uniform'\n",
    "private =   False\n",
    "hyperparameter_tuning = False\n",
    "C = 1\n",
    "\n",
    "\n",
    "fl_params = {\n",
    "    'client_num': 2,\n",
    "    'tot_T': 50, \n",
    "    'C': C,\n",
    "    'local_sets': local_sets,\n",
    "    'test_sets': test_sets,\n",
    "    'public_set': public_set,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 3, \n",
    "    'lr': 0.005,\n",
    "    'aggregate': aggregation_method, # 'grads', 'compressed_soft_labels', 'soft_labels'\n",
    "    'hyperparameter_tuning': hyperparameter_tuning, \n",
    "    'weighting': weighting, # 'uniform', 'performance_based'\n",
    "    'default_client_id': 1, \n",
    "    'augment': aug, \n",
    "    'private': private,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'delta': 1e-4,\n",
    "    'epsilon': 5,\n",
    "    'local_benchmark_epochs': 70, \n",
    "    'initial_pub_alignment_epochs': initial_pub_alignment_epochs\n",
    "}\n",
    "\n",
    "N_pub = len(fl_params['public_set'][0])\n",
    "exp_path = f\"../fl_results/{dataset}/DP{private}/N_pub{N_pub}/Agg{fl_params['aggregate']}_C{fl_params['C']}_HT{fl_params['hyperparameter_tuning']}_Aug{fl_params['augment']}_W{fl_params['weighting']}\"\n",
    "fl_params['exp_path'] = exp_path\n",
    "server = FLServer(fl_params)\n",
    "\n",
    "FL_acc = []\n",
    "for t in range(fl_params['tot_T']):\n",
    "    avg_acc, min_acc, max_acc, avg_train_acc = server.global_update()\n",
    "    FL_acc.append(avg_acc)\n",
    "    print(f\"Round {t} accuracy: avg: {avg_acc} min: {min_acc}  max: {max_acc} avg_train: {avg_train_acc}\")\n",
    "print(\"Final accuracy: \", FL_acc[-1])\n",
    "print() \n",
    "\n",
    "# client_accs = []\n",
    "# for c, client in enumerate(server.clients):\n",
    "#     acc = client.local_benchmark()\n",
    "#     client_accs.append(acc)\n",
    "#     print(f\"Client {c} local benchmark accuracy: {acc}\")\n",
    "# print(\"Client accuracies: \", client_accs, \"  \", np.mean(client_accs))\n",
    "# print()\n",
    "\n",
    "server.save_assets()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
