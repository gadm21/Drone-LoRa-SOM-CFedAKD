{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gadmohamed/miniforge3/envs/env_tf/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from data_utils import * \n",
    "from model_utils import *\n",
    "from utils import FLClient, FLServer\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import os\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CIFAR-10 for FL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "(50000, 32, 32, 3) (50000,) (10000, 32, 32, 3) (10000,) (50000, 10) (10000, 10)\n",
      "(50000, 32, 32, 3) (50000,) (10000, 32, 32, 3) (10000,) (50000, 10) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "dataset = 'cifar10'\n",
    "num_classes = 10 if dataset == 'cifar10' else 100 \n",
    "pub_num_classes = 100 if num_classes == 10 else 10\n",
    "datadir = '../data'\n",
    "partition = 'iid' \n",
    "n_parties = 5\n",
    "beta = 0.5\n",
    "\n",
    "(X_train, y_train, X_test, y_test, net_dataidx_map) = partition_data('cifar10', datadir=datadir, partition = partition, n_parties = n_parties, beta = beta)\n",
    "(X_train_public, y_train_public, X_test_public, y_test_public, net_dataidx_map_public) = partition_data('cifar100', datadir=datadir, partition = 'iid', n_parties = 10, beta = 0.5)\n",
    "# divide y_public by 10 to make it compatible with cifar10\n",
    "y_train_public = y_train_public // 10\n",
    "y_test_public = y_test_public // 10\n",
    "\n",
    "y_train_cat = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "# use num_classes instead of pub_num_classes to make it compatible with cifar10\n",
    "y_train_public_cat = to_categorical(y_train_public, num_classes=num_classes)\n",
    "y_test_public_cat = to_categorical(y_test_public, num_classes=num_classes)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, y_train_cat.shape, y_test_cat.shape)\n",
    "print(X_train_public.shape, y_train_public.shape, X_test_public.shape, y_test_public.shape, y_train_public_cat.shape, y_test_public_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 32, 32, 3) (5000, 10)\n",
      "client  0   (10000, 32, 32, 3) (10000, 10)\n",
      "(10000, 32, 32, 3) (10000, 10)\n",
      "\n",
      "client  1   (10000, 32, 32, 3) (10000, 10)\n",
      "(10000, 32, 32, 3) (10000, 10)\n",
      "\n",
      "client  2   (10000, 32, 32, 3) (10000, 10)\n",
      "(10000, 32, 32, 3) (10000, 10)\n",
      "\n",
      "client  3   (10000, 32, 32, 3) (10000, 10)\n",
      "(10000, 32, 32, 3) (10000, 10)\n",
      "\n",
      "client  4   (10000, 32, 32, 3) (10000, 10)\n",
      "(10000, 32, 32, 3) (10000, 10)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "local_sets = [] \n",
    "test_sets = []\n",
    "public_set = (X_train_public[net_dataidx_map_public[0]], y_train_public_cat[net_dataidx_map_public[0]])\n",
    "for i in range(n_parties):\n",
    "    local_sets.append((X_train[net_dataidx_map[i]], y_train_cat[net_dataidx_map[i]]))\n",
    "    test_sets.append((X_test, y_test_cat))\n",
    "    \n",
    "print(public_set[0].shape, public_set[1].shape)\n",
    "for i in range(n_parties):\n",
    "    print('client ', i, ' ', local_sets[i][0].shape, local_sets[i][1].shape)\n",
    "    print(test_sets[i][0].shape, test_sets[i][1].shape)\n",
    "    print() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_method = 'weights'\n",
    "aug = False\n",
    "weighting = 'uniform'\n",
    "private =   False\n",
    "hyperparameter_tuning = False\n",
    "C = 1\n",
    "initial_pub_alignment_epochs = 4\n",
    "\n",
    "fl_params = {\n",
    "    'client_num': 5, \n",
    "    'tot_T': 40, \n",
    "    'C': C,\n",
    "    'local_sets': local_sets,\n",
    "    'test_sets': test_sets,\n",
    "    'public_set': public_set,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 10, \n",
    "    'lr': 0.003,\n",
    "    'aggregate': aggregation_method, # 'grads', 'compressed_soft_labels', 'soft_labels'\n",
    "    'hyperparameter_tuning': hyperparameter_tuning, \n",
    "    'weighting': weighting, # 'uniform', 'performance_based'\n",
    "    'default_client_id': 1, \n",
    "    'augment': aug, \n",
    "    'private': private,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'delta': 1e-4,\n",
    "    'epsilon': 5,\n",
    "    'local_benchmark_epochs': 70,\n",
    "    'initial_pub_alignment_epochs': initial_pub_alignment_epochs\n",
    "}\n",
    "\n",
    "N_pub = len(fl_params['public_set'][0])\n",
    "exp_path = f\"../fl_results/{dataset}/DP{private}/N_pub{N_pub}/Agg{fl_params['aggregate']}_C{fl_params['C']}_HT{fl_params['hyperparameter_tuning']}_Aug{fl_params['augment']}_W{fl_params['weighting']}\"\n",
    "fl_params['exp_path'] = exp_path\n",
    "server = FLServer(fl_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train acc 0.10403354632587859 | Test acc 0.10233626198083066\n",
      "Epoch 1 | Train acc 0.1255990415335463 | Test acc 0.13688099041533547\n",
      "Epoch 2 | Train acc 0.15724840255591055 | Test acc 0.18440495207667731\n",
      "Epoch 3 | Train acc 0.17911341853035143 | Test acc 0.17452076677316294\n",
      "Epoch 4 | Train acc 0.17821485623003194 | Test acc 0.19219249201277955\n",
      "Epoch 5 | Train acc 0.19788338658146964 | Test acc 0.2134584664536741\n",
      "Epoch 6 | Train acc 0.22613817891373802 | Test acc 0.23452476038338657\n",
      "Epoch 7 | Train acc 0.2382188498402556 | Test acc 0.24880191693290735\n",
      "Epoch 8 | Train acc 0.2534944089456869 | Test acc 0.2698682108626198\n",
      "Epoch 9 | Train acc 0.2663738019169329 | Test acc 0.2760583067092652\n",
      "Epoch 0 | Train acc 0.0950479233226837 | Test acc 0.10123801916932908\n",
      "Epoch 1 | Train acc 0.12649760383386582 | Test acc 0.14946086261980832\n",
      "Epoch 2 | Train acc 0.16333865814696485 | Test acc 0.16064297124600638\n",
      "Epoch 3 | Train acc 0.17192492012779553 | Test acc 0.17202476038338657\n",
      "Epoch 4 | Train acc 0.18849840255591055 | Test acc 0.1951876996805112\n",
      "Epoch 5 | Train acc 0.19988019169329074 | Test acc 0.21605431309904152\n",
      "Epoch 6 | Train acc 0.22494009584664537 | Test acc 0.23532348242811502\n",
      "Epoch 7 | Train acc 0.24031549520766773 | Test acc 0.24490814696485624\n",
      "Epoch 8 | Train acc 0.25029952076677314 | Test acc 0.2608825878594249\n",
      "Epoch 9 | Train acc 0.2662739616613419 | Test acc 0.28065095846645366\n",
      "Epoch 0 | Train acc 0.10892571884984026 | Test acc 0.12310303514376997\n",
      "Epoch 1 | Train acc 0.12160543130990416 | Test acc 0.11751198083067092\n",
      "Epoch 2 | Train acc 0.11491613418530351 | Test acc 0.14147364217252395\n",
      "Epoch 3 | Train acc 0.1610423322683706 | Test acc 0.17412140575079874\n",
      "Epoch 4 | Train acc 0.18819888178913738 | Test acc 0.21665335463258786\n",
      "Epoch 5 | Train acc 0.21785143769968052 | Test acc 0.22803514376996806\n",
      "Epoch 6 | Train acc 0.2240415335463259 | Test acc 0.2397164536741214\n",
      "Epoch 7 | Train acc 0.23083067092651757 | Test acc 0.23901757188498401\n",
      "Epoch 8 | Train acc 0.24770367412140576 | Test acc 0.2498003194888179\n",
      "Epoch 9 | Train acc 0.2630790734824281 | Test acc 0.2787539936102236\n",
      "Epoch 0 | Train acc 0.10193690095846646 | Test acc 0.10293530351437699\n",
      "Epoch 1 | Train acc 0.10672923322683706 | Test acc 0.1064297124600639\n",
      "Epoch 2 | Train acc 0.1326876996805112 | Test acc 0.15285543130990414\n",
      "Epoch 3 | Train acc 0.17442092651757188 | Test acc 0.18360623003194887\n",
      "Epoch 4 | Train acc 0.19828274760383385 | Test acc 0.1979832268370607\n",
      "Epoch 5 | Train acc 0.2073682108626198 | Test acc 0.21076277955271566\n",
      "Epoch 6 | Train acc 0.2104632587859425 | Test acc 0.23113019169329074\n",
      "Epoch 7 | Train acc 0.22384185303514376 | Test acc 0.23412539936102236\n",
      "Epoch 8 | Train acc 0.24011581469648563 | Test acc 0.24550718849840256\n",
      "Epoch 9 | Train acc 0.24730431309904152 | Test acc 0.240814696485623\n",
      "Epoch 0 | Train acc 0.12450079872204473 | Test acc 0.13807907348242812\n",
      "Epoch 1 | Train acc 0.1505591054313099 | Test acc 0.16363817891373802\n",
      "Epoch 2 | Train acc 0.1619408945686901 | Test acc 0.1551517571884984\n",
      "Epoch 3 | Train acc 0.1639376996805112 | Test acc 0.1678314696485623\n",
      "Epoch 4 | Train acc 0.17841453674121405 | Test acc 0.20537140575079874\n",
      "Epoch 5 | Train acc 0.20447284345047922 | Test acc 0.21036341853035143\n",
      "Epoch 6 | Train acc 0.21425718849840256 | Test acc 0.22513977635782748\n",
      "Epoch 7 | Train acc 0.22454073482428116 | Test acc 0.2358226837060703\n",
      "Epoch 8 | Train acc 0.23592252396166133 | Test acc 0.24910143769968052\n",
      "Epoch 9 | Train acc 0.24820287539936103 | Test acc 0.26168130990415334\n",
      "Round 0 : avg_acc 0.26759185303514377, min_acc 0.240814696485623, max_acc 0.28065095846645366, avg_train_acc 0.2582468051118211\n",
      "Epoch 0 | Train acc 0.14966054313099042 | Test acc 0.15045926517571884\n",
      "Epoch 1 | Train acc 0.1499600638977636 | Test acc 0.15135782747603835\n",
      "Epoch 2 | Train acc 0.1556509584664537 | Test acc 0.152555910543131\n",
      "Epoch 3 | Train acc 0.16882987220447285 | Test acc 0.17671725239616615\n",
      "Epoch 4 | Train acc 0.1781150159744409 | Test acc 0.1822084664536741\n",
      "Epoch 5 | Train acc 0.17971246006389777 | Test acc 0.16922923322683706\n",
      "Epoch 6 | Train acc 0.1726238019169329 | Test acc 0.16833067092651757\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/gadmohamed/Desktop/live repos/Drone-LoRa-SOM-CFedAKD/fl_src/doing_fl.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gadmohamed/Desktop/live%20repos/Drone-LoRa-SOM-CFedAKD/fl_src/doing_fl.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m rr \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(fl_params[\u001b[39m'\u001b[39m\u001b[39mtot_T\u001b[39m\u001b[39m'\u001b[39m]) : \n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gadmohamed/Desktop/live%20repos/Drone-LoRa-SOM-CFedAKD/fl_src/doing_fl.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     avg_acc, min_acc, max_acc, avg_train_acc \u001b[39m=\u001b[39m server\u001b[39m.\u001b[39;49mglobal_update(verbose \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gadmohamed/Desktop/live%20repos/Drone-LoRa-SOM-CFedAKD/fl_src/doing_fl.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRound \u001b[39m\u001b[39m{\u001b[39;00mrr\u001b[39m}\u001b[39;00m\u001b[39m : avg_acc \u001b[39m\u001b[39m{\u001b[39;00mavg_acc\u001b[39m}\u001b[39;00m\u001b[39m, min_acc \u001b[39m\u001b[39m{\u001b[39;00mmin_acc\u001b[39m}\u001b[39;00m\u001b[39m, max_acc \u001b[39m\u001b[39m{\u001b[39;00mmax_acc\u001b[39m}\u001b[39;00m\u001b[39m, avg_train_acc \u001b[39m\u001b[39m{\u001b[39;00mavg_train_acc\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/live repos/Drone-LoRa-SOM-CFedAKD/fl_src/utils.py:880\u001b[0m, in \u001b[0;36mFLServer.global_update\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m    876\u001b[0m idxs_users \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclients)), \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mC \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclients)), replace\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)        \n\u001b[1;32m    878\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m idxs_users:\n\u001b[1;32m    879\u001b[0m     \u001b[39m# print(\"client {} accuracy before: {}\".format(idx, self.clients[idx].get_test_acc()))\u001b[39;00m\n\u001b[0;32m--> 880\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclients[idx]\u001b[39m.\u001b[39;49mlocal_update(verbose \u001b[39m=\u001b[39;49m verbose)\n\u001b[1;32m    881\u001b[0m     \u001b[39m# print(\"client {} accuracy after: {} \".format(idx, self.clients[idx].get_test_acc()))\u001b[39;00m\n\u001b[1;32m    884\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39maggregate\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39msoft_labels\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/live repos/Drone-LoRa-SOM-CFedAKD/fl_src/utils.py:652\u001b[0m, in \u001b[0;36mFLClient.local_update\u001b[0;34m(self, evaluate, verbose)\u001b[0m\n\u001b[1;32m    650\u001b[0m log_probs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlog(probs)\n\u001b[1;32m    651\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnllloss(log_probs, y\u001b[39m.\u001b[39margmax(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m--> 652\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    654\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    655\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for rr in range(fl_params['tot_T']) : \n",
    "    avg_acc, min_acc, max_acc, avg_train_acc = server.global_update(verbose = True)\n",
    "    print(f\"Round {rr} : avg_acc {avg_acc}, min_acc {min_acc}, max_acc {max_acc}, avg_train_acc {avg_train_acc}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# server = FLServer(fl_params)\n",
    "# for i in range(4) : \n",
    "#     accs, losses = train(server.clients[0].model, server.clients[0].local_dl, server.clients[0].optimizer)\n",
    "#     print(f\"Round {i} : accs {accs}, losses {losses}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_method = 'soft_labels'\n",
    "aug = False\n",
    "weighting = 'uniform'\n",
    "private =   False\n",
    "hyperparameter_tuning = False\n",
    "C = 1\n",
    "initial_pub_alignment_epochs = 4 \n",
    "\n",
    "fl_params = {\n",
    "    'client_num': 5, \n",
    "    'tot_T': 40, \n",
    "    'C': C,\n",
    "    'local_sets': local_sets,\n",
    "    'test_sets': test_sets,\n",
    "    'public_set': public_set,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 10, \n",
    "    'lr': 0.003,\n",
    "    'aggregate': aggregation_method, # 'grads', 'compressed_soft_labels', 'soft_labels'\n",
    "    'hyperparameter_tuning': hyperparameter_tuning, \n",
    "    'weighting': weighting, # 'uniform', 'performance_based'\n",
    "    'default_client_id': 1, \n",
    "    'augment': aug, \n",
    "    'private': private,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'delta': 1e-4,\n",
    "    'epsilon': 5,\n",
    "    'local_benchmark_epochs': 70,\n",
    "    'initial_pub_alignment_epochs': initial_pub_alignment_epochs\n",
    "}\n",
    "\n",
    "N_pub = len(fl_params['public_set'][0])\n",
    "exp_path = f\"../fl_results/{dataset}/DP{private}/N_pub{N_pub}/Agg{fl_params['aggregate']}_C{fl_params['C']}_HT{fl_params['hyperparameter_tuning']}_Aug{fl_params['augment']}_W{fl_params['weighting']}\"\n",
    "fl_params['exp_path'] = exp_path\n",
    "soft_labels_server = FLServer(fl_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rr in range(fl_params['tot_T']) : \n",
    "    avg_acc, min_acc, max_acc, avg_train_acc = soft_labels_server.global_update(verbose = True) \n",
    "    print(f\"Round {rr} : avg_acc {avg_acc}, min_acc {min_acc}, max_acc {max_acc}, avg_train_acc {avg_train_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import size_of \n",
    "soft_labels = server.clients[0].get_soft_labels()\n",
    "compressed_soft_labels = soft_labels_server.clients[0].get_soft_labels(normalize = True, compress = True)\n",
    "clientmodel = server.clients[0].model\n",
    "ssize = size_of(soft_labels, size = 'KB')\n",
    "cssize = size_of(compressed_soft_labels, size = 'KB')\n",
    "clientmodel_state = clientmodel.state_dict()\n",
    "msize = size_of(clientmodel_state, size = 'KB')\n",
    "\n",
    "cssize, ssize, msize \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FedAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_method = 'weights'\n",
    "aug = False\n",
    "weighting = 'uniform'\n",
    "private =   False\n",
    "hyperparameter_tuning = False\n",
    "C = 1\n",
    "\n",
    "fl_params = {\n",
    "    'client_num': len(local_sets),\n",
    "    'tot_T': 50, \n",
    "    'C': C,\n",
    "    'local_sets': local_sets,\n",
    "    'test_sets': test_sets,\n",
    "    'public_set': public_set,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 3, \n",
    "    'lr': 0.005,\n",
    "    'aggregate': aggregation_method, # 'grads', 'compressed_soft_labels', 'soft_labels'\n",
    "    'hyperparameter_tuning': hyperparameter_tuning, \n",
    "    'weighting': weighting, # 'uniform', 'performance_based'\n",
    "    'default_client_id': 1, \n",
    "    'augment': aug, \n",
    "    'private': private,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'delta': 1e-4,\n",
    "    'epsilon': 5,\n",
    "    'local_benchmark_epochs': 70\n",
    "}\n",
    "\n",
    "N_pub = len(fl_params['public_set'][0])\n",
    "exp_path = f\"../fl_results/{dataset}/DP{private}/N_pub{N_pub}/Agg{fl_params['aggregate']}_C{fl_params['C']}_HT{fl_params['hyperparameter_tuning']}_Aug{fl_params['augment']}_W{fl_params['weighting']}\"\n",
    "fl_params['exp_path'] = exp_path\n",
    "server = FLServer(fl_params)\n",
    "\n",
    "FL_acc = []\n",
    "for t in range(fl_params['tot_T']):\n",
    "    avg_acc, min_acc, max_acc, avg_train_acc = server.global_update()\n",
    "    FL_acc.append(avg_acc)\n",
    "    print(f\"Round {t} accuracy: avg: {avg_acc} min: {min_acc}  max: {max_acc} avg_train: {avg_train_acc}\")\n",
    "print(\"Final accuracy: \", FL_acc[-1])\n",
    "print() \n",
    "\n",
    "client_accs = []\n",
    "for c, client in enumerate(server.clients):\n",
    "    acc = client.local_benchmark()\n",
    "    client_accs.append(acc)\n",
    "    print(f\"Client {c} local benchmark accuracy: {acc}\")\n",
    "print(\"Client accuracies: \", client_accs, \"  \", np.mean(client_accs))\n",
    "print()\n",
    "\n",
    "server.save_assets()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FedMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_method = 'soft_labels'\n",
    "aug = False\n",
    "weighting = 'uniform'\n",
    "private =   False\n",
    "hyperparameter_tuning = False\n",
    "C = 1\n",
    "\n",
    "\n",
    "fl_params = {\n",
    "    'client_num': len(local_sets),\n",
    "    'tot_T': 50, \n",
    "    'C': C,\n",
    "    'local_sets': local_sets,\n",
    "    'test_sets': test_sets,\n",
    "    'public_set': public_set,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 3, \n",
    "    'lr': 0.005,\n",
    "    'aggregate': aggregation_method, # 'grads', 'compressed_soft_labels', 'soft_labels'\n",
    "    'hyperparameter_tuning': hyperparameter_tuning, \n",
    "    'weighting': weighting, # 'uniform', 'performance_based'\n",
    "    'default_client_id': 1, \n",
    "    'augment': aug, \n",
    "    'private': private,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'delta': 1e-4,\n",
    "    'epsilon': 5,\n",
    "    'local_benchmark_epochs': 70\n",
    "}\n",
    "\n",
    "N_pub = len(fl_params['public_set'][0])\n",
    "exp_path = f\"../fl_results/{dataset}/DP{private}/N_pub{N_pub}/Agg{fl_params['aggregate']}_C{fl_params['C']}_HT{fl_params['hyperparameter_tuning']}_Aug{fl_params['augment']}_W{fl_params['weighting']}\"\n",
    "fl_params['exp_path'] = exp_path\n",
    "server = FLServer(fl_params)\n",
    "\n",
    "FL_acc = []\n",
    "for t in range(fl_params['tot_T']):\n",
    "    avg_acc, min_acc, max_acc, avg_train_acc = server.global_update()\n",
    "    FL_acc.append(avg_acc)\n",
    "    print(f\"Round {t} accuracy: avg: {avg_acc} min: {min_acc}  max: {max_acc} avg_train: {avg_train_acc}\")\n",
    "print(\"Final accuracy: \", FL_acc[-1])\n",
    "print() \n",
    "\n",
    "# client_accs = []\n",
    "# for c, client in enumerate(server.clients):\n",
    "#     acc = client.local_benchmark()\n",
    "#     client_accs.append(acc)\n",
    "#     print(f\"Client {c} local benchmark accuracy: {acc}\")\n",
    "# print(\"Client accuracies: \", client_accs, \"  \", np.mean(client_accs))\n",
    "# print()\n",
    "\n",
    "server.save_assets()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CFedAKD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_method = 'compressed_soft_labels'\n",
    "aug = True\n",
    "weighting = 'uniform'\n",
    "private =   False\n",
    "hyperparameter_tuning = False\n",
    "C = 1\n",
    "\n",
    "\n",
    "fl_params = {\n",
    "    'client_num': len(local_sets),\n",
    "    'tot_T': 50, \n",
    "    'C': C,\n",
    "    'local_sets': local_sets,\n",
    "    'test_sets': test_sets,\n",
    "    'public_set': public_set,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 3, \n",
    "    'lr': 0.005,\n",
    "    'aggregate': aggregation_method, # 'grads', 'compressed_soft_labels', 'soft_labels'\n",
    "    'hyperparameter_tuning': hyperparameter_tuning, \n",
    "    'weighting': weighting, # 'uniform', 'performance_based'\n",
    "    'default_client_id': 1, \n",
    "    'augment': aug, \n",
    "    'private': private,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'delta': 1e-4,\n",
    "    'epsilon': 5,\n",
    "    'local_benchmark_epochs': 70\n",
    "}\n",
    "\n",
    "N_pub = len(fl_params['public_set'][0])\n",
    "exp_path = f\"../fl_results/{dataset}/DP{private}/N_pub{N_pub}/Agg{fl_params['aggregate']}_C{fl_params['C']}_HT{fl_params['hyperparameter_tuning']}_Aug{fl_params['augment']}_W{fl_params['weighting']}\"\n",
    "fl_params['exp_path'] = exp_path\n",
    "server = FLServer(fl_params)\n",
    "\n",
    "FL_acc = []\n",
    "for t in range(fl_params['tot_T']):\n",
    "    avg_acc, min_acc, max_acc, avg_train_acc = server.global_update()\n",
    "    FL_acc.append(avg_acc)\n",
    "    print(f\"Round {t} accuracy: avg: {avg_acc} min: {min_acc}  max: {max_acc} avg_train: {avg_train_acc}\")\n",
    "print(\"Final accuracy: \", FL_acc[-1])\n",
    "print() \n",
    "\n",
    "# client_accs = []\n",
    "# for c, client in enumerate(server.clients):\n",
    "#     acc = client.local_benchmark()\n",
    "#     client_accs.append(acc)\n",
    "#     print(f\"Client {c} local benchmark accuracy: {acc}\")\n",
    "# print(\"Client accuracies: \", client_accs, \"  \", np.mean(client_accs))\n",
    "# print()\n",
    "\n",
    "server.save_assets()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FedAKD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_method = 'soft_labels'\n",
    "aug = True\n",
    "weighting = 'uniform'\n",
    "private =   False\n",
    "hyperparameter_tuning = False\n",
    "C = 1\n",
    "\n",
    "\n",
    "fl_params = {\n",
    "    'client_num': 2,\n",
    "    'tot_T': 50, \n",
    "    'C': C,\n",
    "    'local_sets': local_sets,\n",
    "    'test_sets': test_sets,\n",
    "    'public_set': public_set,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 3, \n",
    "    'lr': 0.005,\n",
    "    'aggregate': aggregation_method, # 'grads', 'compressed_soft_labels', 'soft_labels'\n",
    "    'hyperparameter_tuning': hyperparameter_tuning, \n",
    "    'weighting': weighting, # 'uniform', 'performance_based'\n",
    "    'default_client_id': 1, \n",
    "    'augment': aug, \n",
    "    'private': private,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'delta': 1e-4,\n",
    "    'epsilon': 5,\n",
    "    'local_benchmark_epochs': 70, \n",
    "    'initial_pub_alignment_epochs': initial_pub_alignment_epochs\n",
    "}\n",
    "\n",
    "N_pub = len(fl_params['public_set'][0])\n",
    "exp_path = f\"../fl_results/{dataset}/DP{private}/N_pub{N_pub}/Agg{fl_params['aggregate']}_C{fl_params['C']}_HT{fl_params['hyperparameter_tuning']}_Aug{fl_params['augment']}_W{fl_params['weighting']}\"\n",
    "fl_params['exp_path'] = exp_path\n",
    "server = FLServer(fl_params)\n",
    "\n",
    "FL_acc = []\n",
    "for t in range(fl_params['tot_T']):\n",
    "    avg_acc, min_acc, max_acc, avg_train_acc = server.global_update()\n",
    "    FL_acc.append(avg_acc)\n",
    "    print(f\"Round {t} accuracy: avg: {avg_acc} min: {min_acc}  max: {max_acc} avg_train: {avg_train_acc}\")\n",
    "print(\"Final accuracy: \", FL_acc[-1])\n",
    "print() \n",
    "\n",
    "# client_accs = []\n",
    "# for c, client in enumerate(server.clients):\n",
    "#     acc = client.local_benchmark()\n",
    "#     client_accs.append(acc)\n",
    "#     print(f\"Client {c} local benchmark accuracy: {acc}\")\n",
    "# print(\"Client accuracies: \", client_accs, \"  \", np.mean(client_accs))\n",
    "# print()\n",
    "\n",
    "server.save_assets()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
