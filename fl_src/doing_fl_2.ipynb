{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOFFgQYK2gOi"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMK_1jzFsW76"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yNpbtBsdsW78"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/gadmohamed/miniforge3/envs/env_tf/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from data_utils import * \n",
        "from model_utils import *\n",
        "from utils import *\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import product\n",
        "import os\n",
        "import json\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o41efumvsW7_"
      },
      "source": [
        "## FedAvg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfVaRKNVuepO",
        "outputId": "29c60d4e-80f8-42b6-d95f-5ecb6a17b143"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "(50000, 32, 32, 3) (50000,) (10000, 32, 32, 3) (10000,) (50000, 10) (10000, 10)\n",
            "(50000, 32, 32, 3) (50000,) (10000, 32, 32, 3) (10000,) (50000, 10) (10000, 10)\n",
            "(5000, 32, 32, 3) (5000, 10)\n",
            "client  0   (10000, 32, 32, 3) (10000, 10)\n",
            "(10000, 32, 32, 3) (10000, 10)\n",
            "\n",
            "client  1   (10000, 32, 32, 3) (10000, 10)\n",
            "(10000, 32, 32, 3) (10000, 10)\n",
            "\n",
            "client  2   (10000, 32, 32, 3) (10000, 10)\n",
            "(10000, 32, 32, 3) (10000, 10)\n",
            "\n",
            "client  3   (10000, 32, 32, 3) (10000, 10)\n",
            "(10000, 32, 32, 3) (10000, 10)\n",
            "\n",
            "client  4   (10000, 32, 32, 3) (10000, 10)\n",
            "(10000, 32, 32, 3) (10000, 10)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dataset = 'cifar10'\n",
        "num_classes = 10 if dataset == 'cifar10' else 100 \n",
        "pub_num_classes = 100 if num_classes == 10 else 10\n",
        "datadir = '../data'\n",
        "partition = 'iid' \n",
        "n_parties = 5\n",
        "beta = 0.5\n",
        "\n",
        "(X_train, y_train, X_test, y_test, net_dataidx_map) = partition_data('cifar10', datadir=datadir, partition = partition, n_parties = n_parties, beta = beta)\n",
        "(X_train_public, y_train_public, X_test_public, y_test_public, net_dataidx_map_public) = partition_data('cifar100', datadir=datadir, partition = 'iid', n_parties = 10, beta = 0.5)\n",
        "# divide y_public by 10 to make it compatible with cifar10\n",
        "y_train_public = y_train_public // 10\n",
        "y_test_public = y_test_public // 10\n",
        "\n",
        "y_train_cat = to_categorical(y_train, num_classes=num_classes)\n",
        "y_test_cat = to_categorical(y_test, num_classes=num_classes)\n",
        "\n",
        "# use num_classes instead of pub_num_classes to make it compatible with cifar10\n",
        "y_train_public_cat = to_categorical(y_train_public, num_classes=num_classes)\n",
        "y_test_public_cat = to_categorical(y_test_public, num_classes=num_classes)\n",
        "\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, y_train_cat.shape, y_test_cat.shape)\n",
        "print(X_train_public.shape, y_train_public.shape, X_test_public.shape, y_test_public.shape, y_train_public_cat.shape, y_test_public_cat.shape)\n",
        "\n",
        "\n",
        "local_sets = [] \n",
        "test_sets = []\n",
        "public_set = (X_train_public[net_dataidx_map_public[0]], y_train_public_cat[net_dataidx_map_public[0]])\n",
        "for i in range(n_parties):\n",
        "    local_sets.append((X_train[net_dataidx_map[i]], y_train_cat[net_dataidx_map[i]]))\n",
        "    test_sets.append((X_test, y_test_cat))\n",
        "    \n",
        "print(public_set[0].shape, public_set[1].shape)\n",
        "for i in range(n_parties):\n",
        "    print('client ', i, ' ', local_sets[i][0].shape, local_sets[i][1].shape)\n",
        "    print(test_sets[i][0].shape, test_sets[i][1].shape)\n",
        "    print() \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrflp5WvsW7_",
        "outputId": "aa7a26dd-bbc7-4991-e7a5-b10dcab2ba78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 : test acc = 0.11%, test loss = 2.2961\n",
            "Epoch 1 : test acc = 0.10%, test loss = 2.2888\n",
            "Epoch 2 : test acc = 0.13%, test loss = 2.2760\n",
            "Epoch 3 : test acc = 0.18%, test loss = 2.2506\n",
            "Epoch 4 : test acc = 0.22%, test loss = 2.1986\n",
            "Epoch 5 : test acc = 0.23%, test loss = 2.1256\n",
            "Epoch 6 : test acc = 0.25%, test loss = 2.0704\n",
            "Epoch 7 : test acc = 0.27%, test loss = 2.0300\n",
            "Epoch 8 : test acc = 0.28%, test loss = 1.9937\n",
            "Epoch 9 : test acc = 0.29%, test loss = 1.9672\n",
            "Epoch 10 : test acc = 0.29%, test loss = 1.9603\n",
            "Epoch 11 : test acc = 0.29%, test loss = 1.9356\n",
            "Epoch 12 : test acc = 0.32%, test loss = 1.9055\n",
            "Epoch 13 : test acc = 0.31%, test loss = 1.8881\n",
            "Epoch 14 : test acc = 0.33%, test loss = 1.8596\n",
            "Epoch 15 : test acc = 0.33%, test loss = 1.8422\n",
            "Epoch 16 : test acc = 0.35%, test loss = 1.8212\n",
            "Epoch 17 : test acc = 0.34%, test loss = 1.8010\n",
            "Epoch 18 : test acc = 0.35%, test loss = 1.7835\n",
            "Epoch 19 : test acc = 0.35%, test loss = 1.7637\n",
            "Epoch 20 : test acc = 0.36%, test loss = 1.7480\n",
            "Epoch 21 : test acc = 0.37%, test loss = 1.7249\n",
            "Epoch 22 : test acc = 0.38%, test loss = 1.7218\n",
            "Epoch 23 : test acc = 0.39%, test loss = 1.6729\n",
            "Epoch 24 : test acc = 0.39%, test loss = 1.6732\n",
            "Epoch 25 : test acc = 0.38%, test loss = 1.6723\n",
            "Epoch 26 : test acc = 0.38%, test loss = 1.6909\n",
            "Epoch 27 : test acc = 0.41%, test loss = 1.6103\n",
            "Epoch 28 : test acc = 0.41%, test loss = 1.6080\n",
            "Epoch 29 : test acc = 0.41%, test loss = 1.6049\n",
            "Epoch 30 : test acc = 0.42%, test loss = 1.5800\n",
            "Epoch 31 : test acc = 0.43%, test loss = 1.5752\n",
            "Epoch 32 : test acc = 0.41%, test loss = 1.6252\n",
            "Epoch 33 : test acc = 0.44%, test loss = 1.5422\n",
            "Epoch 34 : test acc = 0.43%, test loss = 1.5382\n",
            "Epoch 35 : test acc = 0.45%, test loss = 1.5265\n",
            "Epoch 36 : test acc = 0.42%, test loss = 1.5591\n",
            "Epoch 37 : test acc = 0.41%, test loss = 1.6589\n",
            "Epoch 38 : test acc = 0.45%, test loss = 1.4896\n",
            "Epoch 39 : test acc = 0.46%, test loss = 1.4938\n",
            "Epoch 40 : test acc = 0.46%, test loss = 1.5034\n",
            "Epoch 41 : test acc = 0.43%, test loss = 1.5619\n",
            "Epoch 42 : test acc = 0.45%, test loss = 1.5149\n",
            "Epoch 43 : test acc = 0.47%, test loss = 1.4576\n",
            "Epoch 44 : test acc = 0.46%, test loss = 1.4905\n",
            "Epoch 45 : test acc = 0.46%, test loss = 1.4692\n",
            "Epoch 46 : test acc = 0.47%, test loss = 1.4759\n",
            "Epoch 47 : test acc = 0.47%, test loss = 1.4388\n",
            "Epoch 48 : test acc = 0.49%, test loss = 1.4115\n",
            "Epoch 49 : test acc = 0.49%, test loss = 1.4280\n",
            "Epoch 50 : test acc = 0.48%, test loss = 1.4371\n",
            "Epoch 51 : test acc = 0.48%, test loss = 1.4215\n",
            "Epoch 52 : test acc = 0.50%, test loss = 1.4040\n",
            "Epoch 53 : test acc = 0.47%, test loss = 1.4613\n",
            "Epoch 54 : test acc = 0.50%, test loss = 1.3909\n",
            "Epoch 55 : test acc = 0.49%, test loss = 1.4151\n",
            "Epoch 56 : test acc = 0.51%, test loss = 1.3751\n",
            "Epoch 57 : test acc = 0.49%, test loss = 1.4092\n",
            "Epoch 58 : test acc = 0.50%, test loss = 1.3907\n",
            "Epoch 59 : test acc = 0.51%, test loss = 1.3664\n",
            "Epoch 60 : test acc = 0.50%, test loss = 1.3940\n",
            "Epoch 61 : test acc = 0.51%, test loss = 1.3736\n",
            "Epoch 62 : test acc = 0.47%, test loss = 1.5064\n",
            "Epoch 63 : test acc = 0.51%, test loss = 1.3539\n",
            "Epoch 64 : test acc = 0.49%, test loss = 1.3838\n",
            "Epoch 65 : test acc = 0.51%, test loss = 1.3556\n",
            "Epoch 66 : test acc = 0.51%, test loss = 1.3635\n",
            "Epoch 67 : test acc = 0.51%, test loss = 1.3644\n",
            "Epoch 68 : test acc = 0.51%, test loss = 1.3589\n",
            "Epoch 69 : test acc = 0.53%, test loss = 1.3165\n",
            "Epoch 70 : test acc = 0.53%, test loss = 1.3245\n",
            "Epoch 71 : test acc = 0.53%, test loss = 1.2928\n",
            "Epoch 72 : test acc = 0.53%, test loss = 1.2984\n",
            "Epoch 73 : test acc = 0.53%, test loss = 1.2948\n",
            "Epoch 74 : test acc = 0.55%, test loss = 1.2779\n",
            "Epoch 75 : test acc = 0.53%, test loss = 1.3016\n",
            "Epoch 76 : test acc = 0.53%, test loss = 1.3050\n",
            "Epoch 77 : test acc = 0.54%, test loss = 1.2760\n",
            "Epoch 78 : test acc = 0.52%, test loss = 1.3265\n",
            "Epoch 79 : test acc = 0.52%, test loss = 1.3114\n",
            "Epoch 80 : test acc = 0.54%, test loss = 1.2812\n",
            "Epoch 81 : test acc = 0.55%, test loss = 1.2510\n",
            "Epoch 82 : test acc = 0.55%, test loss = 1.2529\n",
            "Epoch 83 : test acc = 0.55%, test loss = 1.2374\n",
            "Epoch 84 : test acc = 0.55%, test loss = 1.2635\n",
            "Epoch 85 : test acc = 0.56%, test loss = 1.2221\n",
            "Epoch 86 : test acc = 0.55%, test loss = 1.2573\n",
            "Epoch 87 : test acc = 0.52%, test loss = 1.3058\n",
            "Epoch 88 : test acc = 0.56%, test loss = 1.2466\n",
            "Epoch 89 : test acc = 0.55%, test loss = 1.2614\n",
            "Epoch 90 : test acc = 0.55%, test loss = 1.2517\n",
            "Epoch 91 : test acc = 0.55%, test loss = 1.2294\n",
            "Epoch 92 : test acc = 0.57%, test loss = 1.1904\n",
            "Epoch 93 : test acc = 0.56%, test loss = 1.2070\n",
            "Epoch 94 : test acc = 0.56%, test loss = 1.2310\n",
            "Epoch 95 : test acc = 0.57%, test loss = 1.2157\n",
            "Epoch 96 : test acc = 0.57%, test loss = 1.1925\n",
            "Epoch 97 : test acc = 0.55%, test loss = 1.2412\n",
            "Epoch 98 : test acc = 0.52%, test loss = 1.3114\n",
            "Epoch 99 : test acc = 0.54%, test loss = 1.2921\n",
            "Epoch 100 : test acc = 0.58%, test loss = 1.1799\n",
            "Epoch 101 : test acc = 0.57%, test loss = 1.2097\n",
            "Epoch 102 : test acc = 0.58%, test loss = 1.1854\n",
            "Epoch 103 : test acc = 0.58%, test loss = 1.1720\n",
            "Epoch 104 : test acc = 0.59%, test loss = 1.1524\n",
            "Epoch 105 : test acc = 0.59%, test loss = 1.1598\n",
            "Epoch 106 : test acc = 0.58%, test loss = 1.1618\n",
            "Epoch 107 : test acc = 0.57%, test loss = 1.2149\n",
            "Epoch 108 : test acc = 0.59%, test loss = 1.1481\n",
            "Epoch 109 : test acc = 0.60%, test loss = 1.1403\n",
            "Epoch 110 : test acc = 0.55%, test loss = 1.2683\n",
            "Epoch 111 : test acc = 0.59%, test loss = 1.1471\n",
            "Epoch 112 : test acc = 0.59%, test loss = 1.1371\n",
            "Epoch 113 : test acc = 0.60%, test loss = 1.1267\n",
            "Epoch 114 : test acc = 0.59%, test loss = 1.1667\n",
            "Epoch 115 : test acc = 0.60%, test loss = 1.1332\n",
            "Epoch 116 : test acc = 0.58%, test loss = 1.1853\n",
            "Epoch 117 : test acc = 0.58%, test loss = 1.1748\n",
            "Epoch 118 : test acc = 0.58%, test loss = 1.1734\n",
            "Epoch 119 : test acc = 0.58%, test loss = 1.1743\n",
            "Epoch 120 : test acc = 0.57%, test loss = 1.2110\n",
            "Epoch 121 : test acc = 0.60%, test loss = 1.1214\n",
            "Epoch 122 : test acc = 0.60%, test loss = 1.1218\n",
            "Epoch 123 : test acc = 0.60%, test loss = 1.1352\n",
            "Epoch 124 : test acc = 0.59%, test loss = 1.1597\n",
            "Epoch 125 : test acc = 0.59%, test loss = 1.1383\n",
            "Epoch 126 : test acc = 0.57%, test loss = 1.2074\n",
            "Epoch 127 : test acc = 0.62%, test loss = 1.0748\n",
            "Epoch 128 : test acc = 0.60%, test loss = 1.1411\n",
            "Epoch 129 : test acc = 0.58%, test loss = 1.1810\n",
            "Epoch 130 : test acc = 0.60%, test loss = 1.1109\n",
            "Epoch 131 : test acc = 0.61%, test loss = 1.0929\n",
            "Epoch 132 : test acc = 0.60%, test loss = 1.1118\n",
            "Epoch 133 : test acc = 0.61%, test loss = 1.1164\n",
            "Epoch 134 : test acc = 0.60%, test loss = 1.1271\n",
            "Epoch 135 : test acc = 0.62%, test loss = 1.0682\n",
            "Epoch 136 : test acc = 0.59%, test loss = 1.1586\n",
            "Epoch 137 : test acc = 0.61%, test loss = 1.0913\n",
            "Epoch 138 : test acc = 0.62%, test loss = 1.0721\n",
            "Epoch 139 : test acc = 0.62%, test loss = 1.0762\n",
            "Epoch 140 : test acc = 0.61%, test loss = 1.1117\n",
            "Epoch 141 : test acc = 0.62%, test loss = 1.0696\n",
            "Epoch 142 : test acc = 0.62%, test loss = 1.0642\n",
            "Epoch 143 : test acc = 0.61%, test loss = 1.1065\n",
            "Epoch 144 : test acc = 0.62%, test loss = 1.0812\n",
            "Epoch 145 : test acc = 0.62%, test loss = 1.0935\n",
            "Epoch 146 : test acc = 0.62%, test loss = 1.0579\n",
            "Epoch 147 : test acc = 0.63%, test loss = 1.0659\n",
            "Epoch 148 : test acc = 0.61%, test loss = 1.1237\n",
            "Epoch 149 : test acc = 0.62%, test loss = 1.0700\n",
            "Epoch 150 : test acc = 0.63%, test loss = 1.0666\n",
            "Epoch 151 : test acc = 0.63%, test loss = 1.0581\n",
            "Epoch 152 : test acc = 0.60%, test loss = 1.1610\n",
            "Epoch 153 : test acc = 0.61%, test loss = 1.1079\n",
            "Epoch 154 : test acc = 0.61%, test loss = 1.1141\n",
            "Epoch 155 : test acc = 0.61%, test loss = 1.1030\n",
            "Epoch 156 : test acc = 0.63%, test loss = 1.0527\n",
            "Epoch 157 : test acc = 0.62%, test loss = 1.1114\n",
            "Epoch 158 : test acc = 0.63%, test loss = 1.0758\n",
            "Epoch 159 : test acc = 0.63%, test loss = 1.0622\n",
            "Epoch 160 : test acc = 0.62%, test loss = 1.0858\n",
            "Epoch 161 : test acc = 0.61%, test loss = 1.1149\n",
            "Epoch 162 : test acc = 0.62%, test loss = 1.0741\n",
            "Epoch 163 : test acc = 0.64%, test loss = 1.0360\n",
            "Epoch 164 : test acc = 0.62%, test loss = 1.0771\n",
            "Epoch 165 : test acc = 0.65%, test loss = 1.0177\n",
            "Epoch 166 : test acc = 0.63%, test loss = 1.0661\n",
            "Epoch 167 : test acc = 0.63%, test loss = 1.0520\n",
            "Epoch 168 : test acc = 0.63%, test loss = 1.0768\n",
            "Epoch 169 : test acc = 0.64%, test loss = 1.0207\n",
            "Epoch 170 : test acc = 0.65%, test loss = 1.0259\n",
            "Epoch 171 : test acc = 0.63%, test loss = 1.0558\n",
            "Epoch 172 : test acc = 0.64%, test loss = 1.0324\n",
            "Epoch 173 : test acc = 0.63%, test loss = 1.0450\n",
            "Epoch 174 : test acc = 0.63%, test loss = 1.0597\n",
            "Epoch 175 : test acc = 0.61%, test loss = 1.1299\n",
            "Epoch 176 : test acc = 0.64%, test loss = 1.0333\n",
            "Epoch 177 : test acc = 0.64%, test loss = 1.0475\n",
            "Epoch 178 : test acc = 0.63%, test loss = 1.0827\n",
            "Epoch 179 : test acc = 0.62%, test loss = 1.1006\n",
            "Client 0 local benchmark accuracy: 0.6164137380191693\n",
            "Epoch 0 : test acc = 0.10%, test loss = 2.2986\n",
            "Epoch 1 : test acc = 0.15%, test loss = 2.2930\n",
            "Epoch 2 : test acc = 0.17%, test loss = 2.2846\n",
            "Epoch 3 : test acc = 0.20%, test loss = 2.2672\n",
            "Epoch 4 : test acc = 0.23%, test loss = 2.2278\n",
            "Epoch 5 : test acc = 0.24%, test loss = 2.1569\n",
            "Epoch 6 : test acc = 0.25%, test loss = 2.0965\n",
            "Epoch 7 : test acc = 0.26%, test loss = 2.0485\n",
            "Epoch 8 : test acc = 0.27%, test loss = 2.0079\n",
            "Epoch 9 : test acc = 0.27%, test loss = 1.9827\n",
            "Epoch 10 : test acc = 0.30%, test loss = 1.9518\n",
            "Epoch 11 : test acc = 0.30%, test loss = 1.9291\n",
            "Epoch 12 : test acc = 0.30%, test loss = 1.9204\n",
            "Epoch 13 : test acc = 0.31%, test loss = 1.8956\n",
            "Epoch 14 : test acc = 0.32%, test loss = 1.8817\n",
            "Epoch 15 : test acc = 0.33%, test loss = 1.8511\n",
            "Epoch 16 : test acc = 0.33%, test loss = 1.8252\n",
            "Epoch 17 : test acc = 0.35%, test loss = 1.8164\n",
            "Epoch 18 : test acc = 0.35%, test loss = 1.7993\n",
            "Epoch 19 : test acc = 0.36%, test loss = 1.7630\n",
            "Epoch 20 : test acc = 0.36%, test loss = 1.7708\n",
            "Epoch 21 : test acc = 0.37%, test loss = 1.7376\n",
            "Epoch 22 : test acc = 0.37%, test loss = 1.7265\n",
            "Epoch 23 : test acc = 0.39%, test loss = 1.6844\n",
            "Epoch 24 : test acc = 0.40%, test loss = 1.6569\n",
            "Epoch 25 : test acc = 0.40%, test loss = 1.6686\n",
            "Epoch 26 : test acc = 0.40%, test loss = 1.6523\n",
            "Epoch 27 : test acc = 0.39%, test loss = 1.6901\n",
            "Epoch 28 : test acc = 0.40%, test loss = 1.6444\n",
            "Epoch 29 : test acc = 0.41%, test loss = 1.6035\n",
            "Epoch 30 : test acc = 0.41%, test loss = 1.6324\n",
            "Epoch 31 : test acc = 0.43%, test loss = 1.5859\n",
            "Epoch 32 : test acc = 0.42%, test loss = 1.5835\n",
            "Epoch 33 : test acc = 0.42%, test loss = 1.5792\n",
            "Epoch 34 : test acc = 0.45%, test loss = 1.5437\n",
            "Epoch 35 : test acc = 0.44%, test loss = 1.5519\n",
            "Epoch 36 : test acc = 0.44%, test loss = 1.5395\n",
            "Epoch 37 : test acc = 0.44%, test loss = 1.5411\n",
            "Epoch 38 : test acc = 0.44%, test loss = 1.5323\n",
            "Epoch 39 : test acc = 0.45%, test loss = 1.5161\n",
            "Epoch 40 : test acc = 0.46%, test loss = 1.5041\n",
            "Epoch 41 : test acc = 0.46%, test loss = 1.4996\n",
            "Epoch 42 : test acc = 0.46%, test loss = 1.4925\n",
            "Epoch 43 : test acc = 0.45%, test loss = 1.5144\n",
            "Epoch 44 : test acc = 0.46%, test loss = 1.4890\n",
            "Epoch 45 : test acc = 0.43%, test loss = 1.5571\n",
            "Epoch 46 : test acc = 0.45%, test loss = 1.4926\n",
            "Epoch 47 : test acc = 0.46%, test loss = 1.5104\n",
            "Epoch 48 : test acc = 0.46%, test loss = 1.4839\n",
            "Epoch 49 : test acc = 0.46%, test loss = 1.5023\n",
            "Epoch 50 : test acc = 0.46%, test loss = 1.4745\n",
            "Epoch 51 : test acc = 0.48%, test loss = 1.4474\n",
            "Epoch 52 : test acc = 0.47%, test loss = 1.4684\n",
            "Epoch 53 : test acc = 0.47%, test loss = 1.4561\n",
            "Epoch 54 : test acc = 0.49%, test loss = 1.4092\n",
            "Epoch 55 : test acc = 0.48%, test loss = 1.4333\n",
            "Epoch 56 : test acc = 0.50%, test loss = 1.3962\n",
            "Epoch 57 : test acc = 0.50%, test loss = 1.3857\n",
            "Epoch 58 : test acc = 0.50%, test loss = 1.3864\n",
            "Epoch 59 : test acc = 0.49%, test loss = 1.4088\n",
            "Epoch 60 : test acc = 0.47%, test loss = 1.4715\n",
            "Epoch 61 : test acc = 0.52%, test loss = 1.3550\n",
            "Epoch 62 : test acc = 0.49%, test loss = 1.4126\n",
            "Epoch 63 : test acc = 0.50%, test loss = 1.3669\n",
            "Epoch 64 : test acc = 0.48%, test loss = 1.4244\n",
            "Epoch 65 : test acc = 0.48%, test loss = 1.4352\n",
            "Epoch 66 : test acc = 0.52%, test loss = 1.3442\n",
            "Epoch 67 : test acc = 0.51%, test loss = 1.3436\n",
            "Epoch 68 : test acc = 0.51%, test loss = 1.3642\n",
            "Epoch 69 : test acc = 0.51%, test loss = 1.3561\n",
            "Epoch 70 : test acc = 0.52%, test loss = 1.3262\n",
            "Epoch 71 : test acc = 0.52%, test loss = 1.3340\n",
            "Epoch 72 : test acc = 0.52%, test loss = 1.3562\n",
            "Epoch 73 : test acc = 0.52%, test loss = 1.3449\n",
            "Epoch 74 : test acc = 0.54%, test loss = 1.2946\n",
            "Epoch 75 : test acc = 0.53%, test loss = 1.3136\n",
            "Epoch 76 : test acc = 0.50%, test loss = 1.4035\n",
            "Epoch 77 : test acc = 0.49%, test loss = 1.4168\n",
            "Epoch 78 : test acc = 0.52%, test loss = 1.3150\n",
            "Epoch 79 : test acc = 0.52%, test loss = 1.3240\n",
            "Epoch 80 : test acc = 0.54%, test loss = 1.2899\n",
            "Epoch 81 : test acc = 0.52%, test loss = 1.3291\n",
            "Epoch 82 : test acc = 0.55%, test loss = 1.2502\n",
            "Epoch 83 : test acc = 0.53%, test loss = 1.2869\n",
            "Epoch 84 : test acc = 0.55%, test loss = 1.2670\n",
            "Epoch 85 : test acc = 0.54%, test loss = 1.2886\n",
            "Epoch 86 : test acc = 0.54%, test loss = 1.3059\n",
            "Epoch 87 : test acc = 0.55%, test loss = 1.2539\n",
            "Epoch 88 : test acc = 0.53%, test loss = 1.2881\n",
            "Epoch 89 : test acc = 0.55%, test loss = 1.2534\n",
            "Epoch 90 : test acc = 0.53%, test loss = 1.3506\n",
            "Epoch 91 : test acc = 0.54%, test loss = 1.2815\n",
            "Epoch 92 : test acc = 0.56%, test loss = 1.2231\n",
            "Epoch 93 : test acc = 0.56%, test loss = 1.2214\n",
            "Epoch 94 : test acc = 0.56%, test loss = 1.2181\n",
            "Epoch 95 : test acc = 0.57%, test loss = 1.2126\n",
            "Epoch 96 : test acc = 0.57%, test loss = 1.2142\n",
            "Epoch 97 : test acc = 0.56%, test loss = 1.2484\n",
            "Epoch 98 : test acc = 0.56%, test loss = 1.2422\n",
            "Epoch 99 : test acc = 0.58%, test loss = 1.1990\n",
            "Epoch 100 : test acc = 0.56%, test loss = 1.2303\n",
            "Epoch 101 : test acc = 0.59%, test loss = 1.1611\n",
            "Epoch 102 : test acc = 0.57%, test loss = 1.2241\n",
            "Epoch 103 : test acc = 0.58%, test loss = 1.1918\n",
            "Epoch 104 : test acc = 0.57%, test loss = 1.1870\n",
            "Epoch 105 : test acc = 0.56%, test loss = 1.2175\n",
            "Epoch 106 : test acc = 0.59%, test loss = 1.1547\n",
            "Epoch 107 : test acc = 0.59%, test loss = 1.1585\n",
            "Epoch 108 : test acc = 0.57%, test loss = 1.1981\n",
            "Epoch 109 : test acc = 0.59%, test loss = 1.1452\n",
            "Epoch 110 : test acc = 0.59%, test loss = 1.1555\n",
            "Epoch 111 : test acc = 0.60%, test loss = 1.1282\n",
            "Epoch 112 : test acc = 0.58%, test loss = 1.1690\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/gadmohamed/Desktop/live repos/Drone-LoRa-SOM-CFedAKD/fl_src/doing_fl_2.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gadmohamed/Desktop/live%20repos/Drone-LoRa-SOM-CFedAKD/fl_src/doing_fl_2.ipynb#X11sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m client_accs \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gadmohamed/Desktop/live%20repos/Drone-LoRa-SOM-CFedAKD/fl_src/doing_fl_2.ipynb#X11sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mfor\u001b[39;00m c, client \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(server\u001b[39m.\u001b[39mclients):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gadmohamed/Desktop/live%20repos/Drone-LoRa-SOM-CFedAKD/fl_src/doing_fl_2.ipynb#X11sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     acc \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mlocal_benchmark(verbose \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gadmohamed/Desktop/live%20repos/Drone-LoRa-SOM-CFedAKD/fl_src/doing_fl_2.ipynb#X11sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     client_accs\u001b[39m.\u001b[39mappend(acc)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gadmohamed/Desktop/live%20repos/Drone-LoRa-SOM-CFedAKD/fl_src/doing_fl_2.ipynb#X11sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mClient \u001b[39m\u001b[39m{\u001b[39;00mc\u001b[39m}\u001b[39;00m\u001b[39m local benchmark accuracy: \u001b[39m\u001b[39m{\u001b[39;00macc\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m~/Desktop/live repos/Drone-LoRa-SOM-CFedAKD/fl_src/utils.py:468\u001b[0m, in \u001b[0;36mFLClient.local_benchmark\u001b[0;34m(self, save_results, verbose)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocal_optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mSGD(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocal_model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_lr)\n\u001b[1;32m    467\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mlocal_benchmark_epochs\u001b[39m\u001b[39m'\u001b[39m]) : \n\u001b[0;32m--> 468\u001b[0m     _, _ \u001b[39m=\u001b[39m train(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlocal_model, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlocal_dl, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlocal_optimizer, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    469\u001b[0m     test_acc, test_loss \u001b[39m=\u001b[39m test(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocal_model, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_dl, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    470\u001b[0m     \u001b[39mif\u001b[39;00m verbose :\n",
            "File \u001b[0;32m~/Desktop/live repos/Drone-LoRa-SOM-CFedAKD/fl_src/model_utils.py:329\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, privacy_engine, DELTA, device, verbose)\u001b[0m\n\u001b[1;32m    326\u001b[0m log_probs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlog(probs)\n\u001b[1;32m    328\u001b[0m loss \u001b[39m=\u001b[39m nllloss(log_probs, y\u001b[39m.\u001b[39margmax(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m--> 329\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    331\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    332\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
            "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
            "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "aggregation_method = 'weights'\n",
        "aug = False\n",
        "weighting = 'uniform'\n",
        "private =   False\n",
        "hyperparameter_tuning = False\n",
        "C = 1\n",
        "initial_pub_alignment_epochs = 4\n",
        "\n",
        "fl_params = {\n",
        "    'client_num': 5, #len(local_sets),\n",
        "    'tot_T': 30, \n",
        "    'C': C,\n",
        "    'local_sets': local_sets,\n",
        "    'test_sets': test_sets,\n",
        "    'public_set': public_set,\n",
        "    'batch_size': 32,\n",
        "    'epochs': 6, \n",
        "    'lr': 0.003,\n",
        "    'aggregate': aggregation_method, # 'grads', 'compressed_soft_labels', 'soft_labels'\n",
        "    'hyperparameter_tuning': hyperparameter_tuning, \n",
        "    'weighting': weighting, # 'uniform', 'performance_based'\n",
        "    'default_client_id': 1, \n",
        "    'augment': aug, \n",
        "    'private': private,\n",
        "    'max_grad_norm': 1.0,\n",
        "    'delta': 1e-4,\n",
        "    'epsilon': 5,\n",
        "    'local_benchmark_epochs': 180, \n",
        "    'initial_pub_alignment_epochs': initial_pub_alignment_epochs, \n",
        "    'temperature': 0.6, \n",
        "}\n",
        "\n",
        "N_pub = len(fl_params['public_set'][0])\n",
        "exp_path = f\"../fl_results/{dataset}/DP{private}/N_pub{N_pub}/Agg{fl_params['aggregate']}_C{fl_params['C']}_HT{fl_params['hyperparameter_tuning']}_Aug{fl_params['augment']}_W{fl_params['weighting']}\"\n",
        "fl_params['exp_path'] = exp_path\n",
        "server = FLServer(fl_params)\n",
        "\n",
        "# FL_acc = []\n",
        "# for t in range(fl_params['tot_T']):\n",
        "#     avg_acc, min_acc, max_acc, avg_train_acc = server.global_update(verbose = True)\n",
        "#     FL_acc.append(avg_acc)\n",
        "#     print(f\"Round {t} accuracy: avg: {avg_acc} min: {min_acc}  max: {max_acc} avg_train: {avg_train_acc}\")\n",
        "# print(\"Final accuracy: \", FL_acc[-1])\n",
        "# print() \n",
        "\n",
        "client_accs = []\n",
        "for c, client in enumerate(server.clients):\n",
        "    acc = client.local_benchmark(verbose = True)\n",
        "    client_accs.append(acc)\n",
        "    print(f\"Client {c} local benchmark accuracy: {acc}\")\n",
        "print(\"Client accuracies: \", client_accs, \"  \", np.mean(client_accs))\n",
        "print()\n",
        "\n",
        "server.save_assets()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Se7eyCYcsW8A"
      },
      "source": [
        "## FedMD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hhwv5WgmZtUR"
      },
      "outputs": [],
      "source": [
        "dataset = 'cifar10'\n",
        "num_classes = 10 if dataset == 'cifar10' else 100 \n",
        "pub_num_classes = 100 if num_classes == 10 else 10\n",
        "datadir = '../data'\n",
        "partition = 'iid' \n",
        "n_parties = 10\n",
        "beta = 0.5\n",
        "\n",
        "(X_train, y_train, X_test, y_test, net_dataidx_map) = partition_data('cifar10', datadir=datadir, partition = partition, n_parties = n_parties, beta = beta)\n",
        "(X_train_public, y_train_public, X_test_public, y_test_public, net_dataidx_map_public) = partition_data('cifar100', datadir=datadir, partition = 'iid', n_parties = 10, beta = 0.5)\n",
        "# divide y_public by 10 to make it compatible with cifar10\n",
        "y_train_public = y_train_public // 10\n",
        "y_test_public = y_test_public // 10\n",
        "\n",
        "y_train_cat = to_categorical(y_train, num_classes=num_classes)\n",
        "y_test_cat = to_categorical(y_test, num_classes=num_classes)\n",
        "\n",
        "# use num_classes instead of pub_num_classes to make it compatible with cifar10\n",
        "y_train_public_cat = to_categorical(y_train_public, num_classes=num_classes)\n",
        "y_test_public_cat = to_categorical(y_test_public, num_classes=num_classes)\n",
        "\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, y_train_cat.shape, y_test_cat.shape)\n",
        "print(X_train_public.shape, y_train_public.shape, X_test_public.shape, y_test_public.shape, y_train_public_cat.shape, y_test_public_cat.shape)\n",
        "\n",
        "\n",
        "local_sets = [] \n",
        "test_sets = []\n",
        "public_set = (X_train_public[net_dataidx_map_public[0]], y_train_public_cat[net_dataidx_map_public[0]])\n",
        "for i in range(n_parties):\n",
        "    local_sets.append((X_train[net_dataidx_map[i]], y_train_cat[net_dataidx_map[i]]))\n",
        "    test_sets.append((X_test, y_test_cat))\n",
        "    \n",
        "print(public_set[0].shape, public_set[1].shape)\n",
        "for i in range(n_parties):\n",
        "    print('client ', i, ' ', local_sets[i][0].shape, local_sets[i][1].shape)\n",
        "    print(test_sets[i][0].shape, test_sets[i][1].shape)\n",
        "    print() \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGXr_rb-sW8A"
      },
      "outputs": [],
      "source": [
        "aggregation_method = 'soft_labels'\n",
        "aug = False\n",
        "weighting = 'uniform'\n",
        "private =   False\n",
        "hyperparameter_tuning = False\n",
        "C = 1\n",
        "initial_pub_alignment_epochs = 4\n",
        "\n",
        "\n",
        "fl_params = {\n",
        "    'client_num': 5, #len(local_sets),\n",
        "    'tot_T': 30, \n",
        "    'C': C,\n",
        "    'local_sets': local_sets,\n",
        "    'test_sets': test_sets,\n",
        "    'public_set': public_set,\n",
        "    'batch_size': 32,\n",
        "    'epochs': 6, \n",
        "    'lr': 0.003,\n",
        "    'aggregate': aggregation_method, # 'grads', 'compressed_soft_labels', 'soft_labels'\n",
        "    'hyperparameter_tuning': hyperparameter_tuning, \n",
        "    'weighting': weighting, # 'uniform', 'performance_based'\n",
        "    'default_client_id': 1, \n",
        "    'augment': aug, \n",
        "    'private': private,\n",
        "    'max_grad_norm': 1.0,\n",
        "    'delta': 1e-4,\n",
        "    'epsilon': 5,\n",
        "    'local_benchmark_epochs': 180, \n",
        "    'initial_pub_alignment_epochs': initial_pub_alignment_epochs, \n",
        "    'temperature': 0.6, \n",
        "}\n",
        "\n",
        "N_pub = len(fl_params['public_set'][0])\n",
        "exp_path = f\"../fl_results/{dataset}/DP{private}/N_pub{N_pub}/Agg{fl_params['aggregate']}_C{fl_params['C']}_HT{fl_params['hyperparameter_tuning']}_Aug{fl_params['augment']}_W{fl_params['weighting']}\"\n",
        "fl_params['exp_path'] = exp_path\n",
        "server = FLServer(fl_params)\n",
        "\n",
        "FL_acc = []\n",
        "for t in range(fl_params['tot_T']):\n",
        "    avg_acc, min_acc, max_acc, avg_train_acc = server.global_update()\n",
        "    FL_acc.append(avg_acc)\n",
        "    print(f\"Round {t} accuracy: avg: {avg_acc} min: {min_acc}  max: {max_acc} avg_train: {avg_train_acc}\")\n",
        "print(\"Final accuracy: \", FL_acc[-1])\n",
        "print() \n",
        "\n",
        "# client_accs = []\n",
        "# for c, client in enumerate(server.clients):\n",
        "#     acc = client.local_benchmark()\n",
        "#     client_accs.append(acc)\n",
        "#     print(f\"Client {c} local benchmark accuracy: {acc}\")\n",
        "# print(\"Client accuracies: \", client_accs, \"  \", np.mean(client_accs))\n",
        "# print()\n",
        "\n",
        "server.save_assets()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZoDHvknsW8B"
      },
      "source": [
        "## CFedAKD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXkQG2kmugzb"
      },
      "outputs": [],
      "source": [
        "dataset = 'cifar10'\n",
        "num_classes = 10 if dataset == 'cifar10' else 100 \n",
        "pub_num_classes = 100 if num_classes == 10 else 10\n",
        "datadir = '../data'\n",
        "partition = 'iid' \n",
        "n_parties = 10\n",
        "beta = 0.5\n",
        "\n",
        "(X_train, y_train, X_test, y_test, net_dataidx_map) = partition_data('cifar10', datadir=datadir, partition = partition, n_parties = n_parties, beta = beta)\n",
        "(X_train_public, y_train_public, X_test_public, y_test_public, net_dataidx_map_public) = partition_data('cifar100', datadir=datadir, partition = 'iid', n_parties = 10, beta = 0.5)\n",
        "# divide y_public by 10 to make it compatible with cifar10\n",
        "y_train_public = y_train_public // 10\n",
        "y_test_public = y_test_public // 10\n",
        "\n",
        "y_train_cat = to_categorical(y_train, num_classes=num_classes)\n",
        "y_test_cat = to_categorical(y_test, num_classes=num_classes)\n",
        "\n",
        "# use num_classes instead of pub_num_classes to make it compatible with cifar10\n",
        "y_train_public_cat = to_categorical(y_train_public, num_classes=num_classes)\n",
        "y_test_public_cat = to_categorical(y_test_public, num_classes=num_classes)\n",
        "\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, y_train_cat.shape, y_test_cat.shape)\n",
        "print(X_train_public.shape, y_train_public.shape, X_test_public.shape, y_test_public.shape, y_train_public_cat.shape, y_test_public_cat.shape)\n",
        "\n",
        "\n",
        "local_sets = [] \n",
        "test_sets = []\n",
        "public_set = (X_train_public[net_dataidx_map_public[0]], y_train_public_cat[net_dataidx_map_public[0]])\n",
        "for i in range(n_parties):\n",
        "    local_sets.append((X_train[net_dataidx_map[i]], y_train_cat[net_dataidx_map[i]]))\n",
        "    test_sets.append((X_test, y_test_cat))\n",
        "    \n",
        "print(public_set[0].shape, public_set[1].shape)\n",
        "for i in range(n_parties):\n",
        "    print('client ', i, ' ', local_sets[i][0].shape, local_sets[i][1].shape)\n",
        "    print(test_sets[i][0].shape, test_sets[i][1].shape)\n",
        "    print() \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gyOTdQhsW8B"
      },
      "outputs": [],
      "source": [
        "aggregation_method = 'compressed_soft_labels'\n",
        "aug = True\n",
        "weighting = 'uniform'\n",
        "private =   False\n",
        "hyperparameter_tuning = False\n",
        "C = 1\n",
        "initial_pub_alignment_epochs = 4\n",
        "\n",
        "fl_params = {\n",
        "    'client_num': 5, #len(local_sets),\n",
        "    'tot_T': 30, \n",
        "    'C': C,\n",
        "    'local_sets': local_sets,\n",
        "    'test_sets': test_sets,\n",
        "    'public_set': public_set,\n",
        "    'batch_size': 32,\n",
        "    'epochs': 6, \n",
        "    'lr': 0.003,\n",
        "    'aggregate': aggregation_method, # 'grads', 'compressed_soft_labels', 'soft_labels'\n",
        "    'hyperparameter_tuning': hyperparameter_tuning, \n",
        "    'weighting': weighting, # 'uniform', 'performance_based'\n",
        "    'default_client_id': 1, \n",
        "    'augment': aug, \n",
        "    'private': private,\n",
        "    'max_grad_norm': 1.0,\n",
        "    'delta': 1e-4,\n",
        "    'epsilon': 5,\n",
        "    'local_benchmark_epochs': 180, \n",
        "    'initial_pub_alignment_epochs': initial_pub_alignment_epochs, \n",
        "    'temperature': 0.6, \n",
        "}\n",
        "\n",
        "N_pub = len(fl_params['public_set'][0])\n",
        "exp_path = f\"../fl_results/{dataset}/DP{private}/N_pub{N_pub}/Agg{fl_params['aggregate']}_C{fl_params['C']}_HT{fl_params['hyperparameter_tuning']}_Aug{fl_params['augment']}_W{fl_params['weighting']}\"\n",
        "fl_params['exp_path'] = exp_path\n",
        "server = FLServer(fl_params)\n",
        "\n",
        "FL_acc = []\n",
        "for t in range(fl_params['tot_T']):\n",
        "    avg_acc, min_acc, max_acc, avg_train_acc = server.global_update()\n",
        "    FL_acc.append(avg_acc)\n",
        "    print(f\"Round {t} accuracy: avg: {avg_acc} min: {min_acc}  max: {max_acc} avg_train: {avg_train_acc}\")\n",
        "print(\"Final accuracy: \", FL_acc[-1])\n",
        "print() \n",
        "\n",
        "client_accs = []\n",
        "for c, client in enumerate(server.clients):\n",
        "    acc = client.local_benchmark()\n",
        "    client_accs.append(acc)\n",
        "    print(f\"Client {c} local benchmark accuracy: {acc}\")\n",
        "print(\"Client accuracies: \", client_accs, \"  \", np.mean(client_accs))\n",
        "print()\n",
        "\n",
        "server.save_assets()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41Q3fXafsW8B"
      },
      "source": [
        "## FedAKD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xy-KNHl5uleQ",
        "outputId": "b878db9f-038c-4f0e-ac95-022200e0738f"
      },
      "outputs": [],
      "source": [
        "dataset = 'cifar10'\n",
        "num_classes = 10 if dataset == 'cifar10' else 100 \n",
        "pub_num_classes = 100 if num_classes == 10 else 10\n",
        "datadir = '../data'\n",
        "partition = 'iid' \n",
        "n_parties = 10\n",
        "beta = 0.5\n",
        "\n",
        "(X_train, y_train, X_test, y_test, net_dataidx_map) = partition_data('cifar10', datadir=datadir, partition = partition, n_parties = n_parties, beta = beta)\n",
        "(X_train_public, y_train_public, X_test_public, y_test_public, net_dataidx_map_public) = partition_data('cifar100', datadir=datadir, partition = 'iid', n_parties = 10, beta = 0.5)\n",
        "# divide y_public by 10 to make it compatible with cifar10\n",
        "y_train_public = y_train_public // 10\n",
        "y_test_public = y_test_public // 10\n",
        "\n",
        "y_train_cat = to_categorical(y_train, num_classes=num_classes)\n",
        "y_test_cat = to_categorical(y_test, num_classes=num_classes)\n",
        "\n",
        "# use num_classes instead of pub_num_classes to make it compatible with cifar10\n",
        "y_train_public_cat = to_categorical(y_train_public, num_classes=num_classes)\n",
        "y_test_public_cat = to_categorical(y_test_public, num_classes=num_classes)\n",
        "\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, y_train_cat.shape, y_test_cat.shape)\n",
        "print(X_train_public.shape, y_train_public.shape, X_test_public.shape, y_test_public.shape, y_train_public_cat.shape, y_test_public_cat.shape)\n",
        "\n",
        "\n",
        "local_sets = [] \n",
        "test_sets = []\n",
        "public_set = (X_train_public[net_dataidx_map_public[0]], y_train_public_cat[net_dataidx_map_public[0]])\n",
        "for i in range(n_parties):\n",
        "    local_sets.append((X_train[net_dataidx_map[i]], y_train_cat[net_dataidx_map[i]]))\n",
        "    test_sets.append((X_test, y_test_cat))\n",
        "    \n",
        "print(public_set[0].shape, public_set[1].shape)\n",
        "for i in range(n_parties):\n",
        "    print('client ', i, ' ', local_sets[i][0].shape, local_sets[i][1].shape)\n",
        "    print(test_sets[i][0].shape, test_sets[i][1].shape)\n",
        "    print() \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "icntbBNUsW8C",
        "outputId": "52b26fff-526d-41f8-dea3-65d5bbfcc30e"
      },
      "outputs": [],
      "source": [
        "aggregation_method = 'soft_labels'\n",
        "aug = True\n",
        "weighting = 'uniform'\n",
        "private =   False\n",
        "hyperparameter_tuning = False\n",
        "C = 1\n",
        "initial_pub_alignment_epochs = 4\n",
        "\n",
        "fl_params = {\n",
        "    'client_num': 5, #len(local_sets),\n",
        "    'tot_T': 30, \n",
        "    'C': C,\n",
        "    'local_sets': local_sets,\n",
        "    'test_sets': test_sets,\n",
        "    'public_set': public_set,\n",
        "    'batch_size': 32,\n",
        "    'epochs': 6, \n",
        "    'lr': 0.003,\n",
        "    'aggregate': aggregation_method, # 'grads', 'compressed_soft_labels', 'soft_labels'\n",
        "    'hyperparameter_tuning': hyperparameter_tuning, \n",
        "    'weighting': weighting, # 'uniform', 'performance_based'\n",
        "    'default_client_id': 1, \n",
        "    'augment': aug, \n",
        "    'private': private,\n",
        "    'max_grad_norm': 1.0,\n",
        "    'delta': 1e-4,\n",
        "    'epsilon': 5,\n",
        "    'local_benchmark_epochs': 180, \n",
        "    'initial_pub_alignment_epochs': initial_pub_alignment_epochs, \n",
        "    'temperature': 0.6, \n",
        "}\n",
        "\n",
        "N_pub = len(fl_params['public_set'][0])\n",
        "exp_path = f\"../fl_results/{dataset}/DP{private}/N_pub{N_pub}/Agg{fl_params['aggregate']}_C{fl_params['C']}_HT{fl_params['hyperparameter_tuning']}_Aug{fl_params['augment']}_W{fl_params['weighting']}\"\n",
        "fl_params['exp_path'] = exp_path\n",
        "server = FLServer(fl_params)\n",
        "\n",
        "FL_acc = []\n",
        "for t in range(fl_params['tot_T']):\n",
        "    avg_acc, min_acc, max_acc, avg_train_acc = server.global_update()\n",
        "    FL_acc.append(avg_acc)\n",
        "    print(f\"Round {t} accuracy: avg: {avg_acc} min: {min_acc}  max: {max_acc} avg_train: {avg_train_acc}\")\n",
        "print(\"Final accuracy: \", FL_acc[-1])\n",
        "print() \n",
        "\n",
        "client_accs = []\n",
        "for c, client in enumerate(server.clients):\n",
        "    acc = client.local_benchmark()\n",
        "    client_accs.append(acc)\n",
        "    print(f\"Client {c} local benchmark accuracy: {acc}\")\n",
        "print(\"Client accuracies: \", client_accs, \"  \", np.mean(client_accs))\n",
        "print()\n",
        "\n",
        "server.save_assets()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ag68zZYIJ5Nm"
      },
      "source": [
        "# central training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7bbUUO7J57Y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define the device to use for training (GPU if available, else CPU)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the transforms to apply to the data\n",
        "transform = transforms.Compose(\n",
        "    [transforms.RandomHorizontalFlip(),\n",
        "     transforms.RandomCrop(32, padding=4),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Load the CIFAR-10 dataset and apply the transforms\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                        download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "def softmax_with_temperature(logits, temperature=1.0):\n",
        "    \"\"\"Applies softmax with temperature scaling.\"\"\"\n",
        "    assert temperature > 0, \"Temperature must be positive.\"\n",
        "    scaled_logits = logits / temperature\n",
        "    return F.softmax(scaled_logits, dim=-1)\n",
        "\n",
        "# Define the network architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "        # self.logsoftmax = nn.LogSoftmax(dim = -1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        # log_probs = self.logsoftmax(x) \n",
        "        return x \n",
        "\n",
        "net = Net().to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "cirterion2 = nn.NLLLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "\n",
        "# Train the network for 10 epochs\n",
        "for epoch in range(10):\n",
        "  running_loss = 0.0\n",
        "  running_accuracy = 0.0\n",
        "  for i, data in enumerate(trainloader, 0):\n",
        "      inputs, labels = data[0].to(device), data[1].to(device)\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      logits = net(inputs)\n",
        "      probs = softmax_with_temperature(logits, temperature = 1.0)\n",
        "      soft_labels = softmax_with_temperature(logits, temperature = 0.7)\n",
        "\n",
        "      # CCE = NLLLoss( log( softmax(x)))\n",
        "      log_probs = torch.log(soft_labels) \n",
        "      loss = cirterion2(log_probs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      running_accuracy += labels.eq(probs.argmax(dim = -1)).sum() / len(labels)\n",
        "      running_loss += loss.item()\n",
        "      \n",
        "  print('[%d, %5d] loss: %.3f  acc: %.2f' %\n",
        "        (epoch + 1, i + 1, running_loss / len(trainloader), running_accuracy / len(trainloader)))\n",
        "  \n",
        "\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKnjj26qguAe"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader \n",
        "from torch.utils.data import TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim \n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "nllloss =nn.NLLLoss()\n",
        "def my_train(model, train_loader, optimizer, privacy_engine = None, DELTA = None, device = None):\n",
        "\n",
        "    if device is None : \n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = model.to(device)\n",
        "    \n",
        "    accs = []\n",
        "    losses = []\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(inputs)\n",
        "\n",
        "        probs = softmax_with_temperature(logits, temperature = 1.0) \n",
        "        soft_labels = softmax_with_temperature(logits, temperature = 0.7) \n",
        "        log_probs = torch.log(probs) \n",
        "        print(\"log probs:\", log_probs.shape, labels.shape)\n",
        "        loss = nllloss(log_probs, labels) \n",
        "        # loss = criterion(probs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        preds = probs.argmax(-1)\n",
        "        n_correct = float(labels.eq(probs.argmax(-1)).sum())\n",
        "        batch_accuracy = n_correct / len(labels)\n",
        "\n",
        "        accs.append(batch_accuracy)\n",
        "        losses.append(float(loss))\n",
        "\n",
        "    if privacy_engine is not None:\n",
        "        # epsilon, best_alpha = optimizer.privacy_engine.get_privacy_spent()  # Yep, we put a pointer to privacy_engine into your optimizer :)\n",
        "        epsilon = privacy_engine.get_epsilon(DELTA)\n",
        " \n",
        "    return np.mean(accs), np.mean(losses)\n",
        "\n",
        "\n",
        "def softmax_with_temperature(logits, temperature=1.0):\n",
        "    \"\"\"Applies softmax with temperature scaling.\"\"\"\n",
        "    assert temperature > 0, \"Temperature must be positive.\"\n",
        "    scaled_logits = logits / temperature\n",
        "    return F.softmax(scaled_logits, dim=-1)\n",
        "    \n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "        # self.softmax = nn.Softmax(dim = 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        logits = self.fc2(x)\n",
        "        # probs = self.softmax(x) \n",
        "        # soft_labels = F.softmax(x/0.7, dim=-1)\n",
        "        return logits\n",
        "\n",
        "\n",
        "model = Net() \n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.RandomHorizontalFlip(),\n",
        "     transforms.RandomCrop(32, padding=4),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                        download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "HOFFgQYK2gOi",
        "ag68zZYIJ5Nm"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "fl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
